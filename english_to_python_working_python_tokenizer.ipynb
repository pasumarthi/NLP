{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "english_to_python1_working_python_tokenizer.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.2"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "1ffbb86e0c3646e2acfa9c2b317eb8cb": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_48426b25e2214526b42edbc4f1463a5d",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_523a147c79614044b0608bde2b59ae2b",
              "IPY_MODEL_6d882c19b7bd4032836e5cb411d230e9"
            ]
          }
        },
        "48426b25e2214526b42edbc4f1463a5d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "523a147c79614044b0608bde2b59ae2b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_1364231d5e34476c9ad574aeff037c0b",
            "_dom_classes": [],
            "description": "",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 1,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 0,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_087ff1f376b248b780cd5619e0c8df7a"
          }
        },
        "6d882c19b7bd4032836e5cb411d230e9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_82fc0d84a7204ccea9ba5569aa208156",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 0/? [00:06&lt;?, ?it/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_4bb888150b9a4814b3469506b92eccf0"
          }
        },
        "1364231d5e34476c9ad574aeff037c0b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "087ff1f376b248b780cd5619e0c8df7a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "82fc0d84a7204ccea9ba5569aa208156": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "4bb888150b9a4814b3469506b92eccf0": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "3c496d3f2d9c4acd82c4587c560d4b11": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_c84ab885cda841ac95924f2dcce193ef",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_cd7ea76de2794dc58c2a34d8f09fcf7d",
              "IPY_MODEL_bb8f6339b1d94035ac8cbe09c895ac8c"
            ]
          }
        },
        "c84ab885cda841ac95924f2dcce193ef": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "cd7ea76de2794dc58c2a34d8f09fcf7d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_a5c1f086cf6e400ba1f473c4c7720db9",
            "_dom_classes": [],
            "description": "100%",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 4727,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 4727,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_5782ba61f872454a8d093f5d5e7f8032"
          }
        },
        "bb8f6339b1d94035ac8cbe09c895ac8c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_87a14bd4b33f470a8a544727b1eae2d5",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 4727/4727 [00:00&lt;00:00, 48029.14it/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_c1ae4050fa94402885251f5d60400b1f"
          }
        },
        "a5c1f086cf6e400ba1f473c4c7720db9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "5782ba61f872454a8d093f5d5e7f8032": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "87a14bd4b33f470a8a544727b1eae2d5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "c1ae4050fa94402885251f5d60400b1f": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "85c24676dc274b2192f0ce3360f4c36a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_4bfe8f7bd8334aa2b5f9724f8ab9ff83",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_825a4e6a504c4079ba9d94ac45d177eb",
              "IPY_MODEL_0ccc5213578743859a2a16060f47faf8"
            ]
          }
        },
        "4bfe8f7bd8334aa2b5f9724f8ab9ff83": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "825a4e6a504c4079ba9d94ac45d177eb": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_3c2196b1e2744ec088a33ddf15cdfd4d",
            "_dom_classes": [],
            "description": "100%",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 4727,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 4727,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_128ba8659dfc4e0dafabaf56266ab7db"
          }
        },
        "0ccc5213578743859a2a16060f47faf8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_12cb5613a07c4a0baee8829ce155cccf",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 4727/4727 [00:01&lt;00:00, 3641.14it/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_0f87b5c865364e649a565d1ac66645ef"
          }
        },
        "3c2196b1e2744ec088a33ddf15cdfd4d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "128ba8659dfc4e0dafabaf56266ab7db": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "12cb5613a07c4a0baee8829ce155cccf": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "0f87b5c865364e649a565d1ac66645ef": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/pasumarthi/NLP/blob/main/english_to_python_working_python_tokenizer.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EzxOJwN7EzFO"
      },
      "source": [
        "# English to Python code generation\n",
        "In this notebook, from english sentence python code is generated\n",
        "\n",
        "The technique used in this project is building Neural Transformer based on paper. Attention is All you Need - https://arxiv.org/pdf/1706.03762.pdf.\n",
        "\n",
        "The Sequnece2Sequnce Model used can be depicted in the diagram below\n",
        "\n",
        "![](https://raw.githubusercontent.com/bentrevett/pytorch-seq2seq/9479fcb532214ad26fd4bda9fcf081a05e1aaf4e/assets/transformer1.png)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2c15qqV0Rs4m",
        "outputId": "f6197508-dc1a-4503-b1ea-f6fc1b48b3a4"
      },
      "source": [
        "# Mount the google drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/gdrive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uUnMLdevEzFT"
      },
      "source": [
        "import torch\n",
        "import torch.nn as n\n",
        "import torch.optim as optim\n",
        "\n",
        "\n",
        "from torchtext.legacy.data import Field, BucketIterator\n",
        "\n",
        "import spacy\n",
        "import numpy as np\n",
        "\n",
        "import random\n",
        "import math\n",
        "import time"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "srzErHAsEzFU"
      },
      "source": [
        "Then set a random seed for deterministic results/reproducability."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r_wP4J4LEzFX"
      },
      "source": [
        "SEED = 1222\n",
        "\n",
        "random.seed(SEED)\n",
        "np.random.seed(SEED)\n",
        "torch.manual_seed(SEED)\n",
        "torch.cuda.manual_seed(SEED)\n",
        "torch.backends.cudnn.deterministic = True"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hawianoFEzFY"
      },
      "source": [
        "Instantiate English spaCy models."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9BP3YSvJEzFY",
        "outputId": "0fa5bdd0-f982-4732-8b30-a3f459cc9693"
      },
      "source": [
        "%%bash\n",
        "python -m spacy download en\n"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: en_core_web_sm==2.2.5 from https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-2.2.5/en_core_web_sm-2.2.5.tar.gz#egg=en_core_web_sm==2.2.5 in /usr/local/lib/python3.7/dist-packages (2.2.5)\n",
            "Requirement already satisfied: spacy>=2.2.2 in /usr/local/lib/python3.7/dist-packages (from en_core_web_sm==2.2.5) (2.2.4)\n",
            "Requirement already satisfied: numpy>=1.15.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (1.19.5)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (2.0.5)\n",
            "Requirement already satisfied: srsly<1.1.0,>=1.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (1.0.5)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (2.23.0)\n",
            "Requirement already satisfied: blis<0.5.0,>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (0.4.1)\n",
            "Requirement already satisfied: thinc==7.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (7.4.0)\n",
            "Requirement already satisfied: wasabi<1.1.0,>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (0.8.2)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (54.1.2)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (3.0.5)\n",
            "Requirement already satisfied: plac<1.2.0,>=0.9.6 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (1.1.3)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (1.0.5)\n",
            "Requirement already satisfied: catalogue<1.1.0,>=0.0.7 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (1.0.0)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (4.41.1)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->en_core_web_sm==2.2.5) (2.10)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->en_core_web_sm==2.2.5) (1.24.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->en_core_web_sm==2.2.5) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->en_core_web_sm==2.2.5) (2020.12.5)\n",
            "Requirement already satisfied: importlib-metadata>=0.20; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from catalogue<1.1.0,>=0.0.7->spacy>=2.2.2->en_core_web_sm==2.2.5) (3.7.2)\n",
            "Requirement already satisfied: typing-extensions>=3.6.4; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=0.20; python_version < \"3.8\"->catalogue<1.1.0,>=0.0.7->spacy>=2.2.2->en_core_web_sm==2.2.5) (3.7.4.3)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=0.20; python_version < \"3.8\"->catalogue<1.1.0,>=0.0.7->spacy>=2.2.2->en_core_web_sm==2.2.5) (3.4.1)\n",
            "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
            "You can now load the model via spacy.load('en_core_web_sm')\n",
            "\u001b[38;5;2m✔ Linking successful\u001b[0m\n",
            "/usr/local/lib/python3.7/dist-packages/en_core_web_sm -->\n",
            "/usr/local/lib/python3.7/dist-packages/spacy/data/en\n",
            "You can now load the model via spacy.load('en')\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EAq6OVJKfXm3"
      },
      "source": [
        "spacy_en = spacy.load('en')"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2bEkyPt5EzFY"
      },
      "source": [
        "Tokenize method is defined for English"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4KaGEZ45EzFZ"
      },
      "source": [
        "import tokenize\n",
        "import io\n",
        "\n",
        "def tokenize_en(text):\n",
        "    \"\"\"\n",
        "    Tokenizes English text from a string into a list of strings\n",
        "    \"\"\"\n",
        "    return [tok.text for tok in spacy_en.tokenizer(text)]\n",
        "\n",
        "def tokenize_python(text):\n",
        "    \"\"\"\n",
        "    Tokenizes Python Code to list of strings\n",
        "    \"\"\"\n",
        "    python_token_list = []\n",
        "    raised_exception = False\n",
        "    try:\n",
        "        tokens = tokenize.tokenize(io.BytesIO(text.encode('utf-8')).readline)\n",
        "        for in_tuple in tokens:\n",
        "            if in_tuple.type == tokenize.COMMENT:\n",
        "                continue\n",
        "            elif in_tuple.type == tokenize.ENCODING:\n",
        "                continue\n",
        "            elif in_tuple.type == tokenize.INDENT:\n",
        "                python_token_list.append(\"INDENT\")\n",
        "            elif in_tuple.type == tokenize.DEDENT:\n",
        "                python_token_list.append(\"DEDENT\")\n",
        "            elif in_tuple.type == tokenize.NL or in_tuple.type == tokenize.NEWLINE:\n",
        "                python_token_list.append(\"NEWLINE\")\n",
        "            elif in_tuple.type == tokenize.ENDMARKER :\n",
        "                continue\n",
        "            else:\n",
        "                python_token_list.append(in_tuple.string)\n",
        "    except Exception:\n",
        "        raised_exception = True\n",
        "        #print( \"Exception: \", Exception, \" program: \", text)\n",
        "    return python_token_list"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2mVT1SFp0aAy"
      },
      "source": [
        "## Download the sample python samples"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0V9qZho8UnAT"
      },
      "source": [
        "from __future__ import absolute_import\n",
        "from __future__ import division\n",
        "from __future__ import print_function\n",
        "from __future__ import unicode_literals\n",
        "\n",
        "import torch\n",
        "from torch.jit import script, trace\n",
        "import torch.nn as nn\n",
        "from torch import optim\n",
        "import torch.nn.functional as F\n",
        "import csv\n",
        "import random\n",
        "import re\n",
        "import os\n",
        "import unicodedata\n",
        "import codecs\n",
        "from io import open\n",
        "import itertools\n",
        "import math\n",
        "\n",
        "\n",
        "USE_CUDA = torch.cuda.is_available()\n",
        "device = torch.device(\"cuda\" if USE_CUDA else \"cpu\")"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tn4KKwW9ov_a",
        "outputId": "1dc95938-caa7-4cb7-9eb8-c4b8b7797019"
      },
      "source": [
        "import nltk\n",
        "import string\n",
        "import re\n",
        "\n",
        "from nltk.corpus import wordnet\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "\n",
        "\n",
        "from nltk.stem.wordnet import WordNetLemmatizer\n",
        "from nltk.corpus import stopwords\n",
        "import pandas as pd\n",
        "import string\n",
        "import nltk\n",
        "import random\n",
        "import random\n",
        "#import google_trans_new\n",
        "#from google_trans_new import google_translator\n",
        "\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "from nltk.corpus import stopwords\n",
        "lem = WordNetLemmatizer()\n",
        "\n",
        "def clean_text(text):\n",
        "    ## lower case\n",
        "    if not isinstance(text, str):\n",
        "      return str(text) \n",
        "    cleaned = text.lower()\n",
        "\n",
        "    urls_pattern = re.compile(r'https?://\\S+|www.\\S+')\n",
        "    cleaned = urls_pattern.sub(r'',cleaned)\n",
        "    \n",
        "    ## remove punctuations\n",
        "    punctuations = string.punctuation\n",
        "    cleaned_temp = \"\".join(character for character in cleaned if character not in punctuations)\n",
        "    \n",
        "    ## remove stopwords \n",
        "    words = cleaned_temp.split()\n",
        "    #stopword_lists = stopwords.words(\"english\")\n",
        "    #cleaned = [word for word in words if word not in stopword_lists]\n",
        "    cleaned = words\n",
        "    \n",
        "    ## normalization - lemmatization\n",
        "    #cleaned = [lem.lemmatize(word, \"v\") for word in cleaned]\n",
        "    #cleaned = [lem.lemmatize(word, \"n\") for word in cleaned]\n",
        "    \n",
        "    ## join \n",
        "    cleaned = \" \".join(cleaned)\n",
        "    return cleaned\n",
        "\n"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/wordnet.zip.\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Unzipping taggers/averaged_perceptron_tagger.zip.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VXmqBI31T5LW"
      },
      "source": [
        "import os\n",
        "os.chdir(\"/content/gdrive/My Drive/NLP_end\")"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MBxOkCU9pZ1a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "681ac2bc-8393-41f8-c57b-b86005e96858"
      },
      "source": [
        "english_text_python_program_pair_list = []\n",
        "process_python_code=False\n",
        "i=1\n",
        "with open('dataset_python_cleaned_final.txt', 'r', encoding=\"utf8\") as f:\n",
        "    for line in f:\n",
        "        #print(i)\n",
        "        i += 1\n",
        "        if process_python_code==False:\n",
        "            if line.strip() == '':\n",
        "                continue\n",
        "            if line.startswith('#'):\n",
        "                english_text = line\n",
        "                #english_text_list.append(line)\n",
        "                process_python_code=True\n",
        "                python_program=''\n",
        "            else:\n",
        "                print(i, \": \", line)            \n",
        "        else:\n",
        "            if line.strip() == '':\n",
        "                process_python_code=False\n",
        "                english_text_python_program_pair_list.append((english_text, python_program))\n",
        "                python_program=''\n",
        "                english_text =''\n",
        "            if line.lstrip().startswith('#'):\n",
        "                continue\n",
        "            else:\n",
        "                python_program += line\n",
        "print(i, \": \", line)                 "
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "37405 :  print(\"Binary Right Shift\", c)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VKvMPHBtrFf_",
        "outputId": "c3fa408f-2bbb-4132-9373-25873519fd8f"
      },
      "source": [
        "len(english_text_python_program_pair_list)"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "4727"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6938tdkLq45F"
      },
      "source": [
        "english_text_list,python_program_list  = zip(*english_text_python_program_pair_list)"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wkK8ux6srNTr"
      },
      "source": [
        "import pandas as pd\n",
        "\n",
        "df = pd.DataFrame({'English': english_text_list, 'Python':python_program_list })"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 195
        },
        "id": "SKeRBdRwrU8Y",
        "outputId": "e1a70d1e-f0dd-400b-b89f-462dd62f64a4"
      },
      "source": [
        "df.head(5)"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>English</th>\n",
              "      <th>Python</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td># write a python program to add two numbers \\n</td>\n",
              "      <td>num1 = 1.5\\nnum2 = 6.3\\nsum = num1 + num2\\npri...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td># write a python function to add two user prov...</td>\n",
              "      <td>def add_two_numbers(num1, num2):\\n    sum = nu...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td># write a program to find and print the larges...</td>\n",
              "      <td>num1 = 10\\nnum2 = 12\\nnum3 = 14\\nif (num1 &gt;= n...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td># write a program to find and print the smalle...</td>\n",
              "      <td>num1 = 10\\nnum2 = 12\\nnum3 = 14\\nif (num1 &lt;= n...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td># Write a python function to merge two given l...</td>\n",
              "      <td>def merge_lists(l1, l2):\\n    return l1 + l2\\n</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                             English                                             Python\n",
              "0     # write a python program to add two numbers \\n  num1 = 1.5\\nnum2 = 6.3\\nsum = num1 + num2\\npri...\n",
              "1  # write a python function to add two user prov...  def add_two_numbers(num1, num2):\\n    sum = nu...\n",
              "2  # write a program to find and print the larges...  num1 = 10\\nnum2 = 12\\nnum3 = 14\\nif (num1 >= n...\n",
              "3  # write a program to find and print the smalle...  num1 = 10\\nnum2 = 12\\nnum3 = 14\\nif (num1 <= n...\n",
              "4  # Write a python function to merge two given l...     def merge_lists(l1, l2):\\n    return l1 + l2\\n"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206,
          "referenced_widgets": [
            "1ffbb86e0c3646e2acfa9c2b317eb8cb",
            "48426b25e2214526b42edbc4f1463a5d",
            "523a147c79614044b0608bde2b59ae2b",
            "6d882c19b7bd4032836e5cb411d230e9",
            "1364231d5e34476c9ad574aeff037c0b",
            "087ff1f376b248b780cd5619e0c8df7a",
            "82fc0d84a7204ccea9ba5569aa208156",
            "4bb888150b9a4814b3469506b92eccf0",
            "3c496d3f2d9c4acd82c4587c560d4b11",
            "c84ab885cda841ac95924f2dcce193ef",
            "cd7ea76de2794dc58c2a34d8f09fcf7d",
            "bb8f6339b1d94035ac8cbe09c895ac8c",
            "a5c1f086cf6e400ba1f473c4c7720db9",
            "5782ba61f872454a8d093f5d5e7f8032",
            "87a14bd4b33f470a8a544727b1eae2d5",
            "c1ae4050fa94402885251f5d60400b1f"
          ]
        },
        "id": "ZbxiaS36rfFQ",
        "outputId": "66659234-1b7e-49e6-e3a7-b446f578d789"
      },
      "source": [
        "from tqdm import tqdm_notebook as tqdm\n",
        "tqdm().pandas() \n",
        "\n",
        "df['English'] = df['English'].progress_apply(lambda txt: clean_text(txt))"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:2: TqdmDeprecationWarning: This function will be removed in tqdm==5.0.0\n",
            "Please use `tqdm.notebook.tqdm` instead of `tqdm.tqdm_notebook`\n",
            "  \n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "1ffbb86e0c3646e2acfa9c2b317eb8cb",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=1.0, bar_style='info', max=1.0), HTML(value='')))"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/tqdm/std.py:658: FutureWarning: The Panel class is removed from pandas. Accessing it from the top-level namespace will also be removed in the next version\n",
            "  from pandas import Panel\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "3c496d3f2d9c4acd82c4587c560d4b11",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=0.0, max=4727.0), HTML(value='')))"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 66,
          "referenced_widgets": [
            "85c24676dc274b2192f0ce3360f4c36a",
            "4bfe8f7bd8334aa2b5f9724f8ab9ff83",
            "825a4e6a504c4079ba9d94ac45d177eb",
            "0ccc5213578743859a2a16060f47faf8",
            "3c2196b1e2744ec088a33ddf15cdfd4d",
            "128ba8659dfc4e0dafabaf56266ab7db",
            "12cb5613a07c4a0baee8829ce155cccf",
            "0f87b5c865364e649a565d1ac66645ef"
          ]
        },
        "id": "V_i2eO9Ys7PF",
        "outputId": "4f8842b8-1762-46a1-d343-8ee80ce89a74"
      },
      "source": [
        "df['Python'] = df['Python'].progress_apply(lambda txt: txt.lstrip())"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "85c24676dc274b2192f0ce3360f4c36a",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=0.0, max=4727.0), HTML(value='')))"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 195
        },
        "id": "0tFqOQ3ZtWLS",
        "outputId": "e86c3fa5-fc15-466c-94bb-badafdcf4656"
      },
      "source": [
        "df.head(5)"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>English</th>\n",
              "      <th>Python</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>write a python program to add two numbers</td>\n",
              "      <td>num1 = 1.5\\nnum2 = 6.3\\nsum = num1 + num2\\npri...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>write a python function to add two user provid...</td>\n",
              "      <td>def add_two_numbers(num1, num2):\\n    sum = nu...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>write a program to find and print the largest ...</td>\n",
              "      <td>num1 = 10\\nnum2 = 12\\nnum3 = 14\\nif (num1 &gt;= n...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>write a program to find and print the smallest...</td>\n",
              "      <td>num1 = 10\\nnum2 = 12\\nnum3 = 14\\nif (num1 &lt;= n...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>write a python function to merge two given lis...</td>\n",
              "      <td>def merge_lists(l1, l2):\\n    return l1 + l2\\n</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                             English                                             Python\n",
              "0          write a python program to add two numbers  num1 = 1.5\\nnum2 = 6.3\\nsum = num1 + num2\\npri...\n",
              "1  write a python function to add two user provid...  def add_two_numbers(num1, num2):\\n    sum = nu...\n",
              "2  write a program to find and print the largest ...  num1 = 10\\nnum2 = 12\\nnum3 = 14\\nif (num1 >= n...\n",
              "3  write a program to find and print the smallest...  num1 = 10\\nnum2 = 12\\nnum3 = 14\\nif (num1 <= n...\n",
              "4  write a python function to merge two given lis...     def merge_lists(l1, l2):\\n    return l1 + l2\\n"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZFrB-gigY70K"
      },
      "source": [
        "import random\n",
        "import torch, torchtext\n",
        "from torchtext import data\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "\n",
        "from torchtext.legacy.data import Field, BucketIterator, Example, Dataset\n",
        "\n",
        "import spacy\n",
        "import numpy as np\n",
        "import math\n",
        "import time"
      ],
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jsuS29qEYqzU"
      },
      "source": [
        "SRC = Field(tokenize=tokenize_en, \n",
        "            init_token='<sos>', \n",
        "            eos_token='<eos>',            \n",
        "            batch_first = True, \n",
        "            lower=True)\n",
        "\n",
        "TRG = Field(tokenize = tokenize_python, \n",
        "            init_token='<sos>', \n",
        "            eos_token='<eos>', \n",
        "            batch_first = True\n",
        "            )"
      ],
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UbA4nkUnY_f7"
      },
      "source": [
        "fields = [('English', SRC),('Python',TRG)]"
      ],
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mZ1D69mYdpB0"
      },
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "train, valid = train_test_split(df, test_size=0.02)"
      ],
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mBIQ_97Vxgsh"
      },
      "source": [
        "train = train.reset_index(drop=True) ## This is being done because data.Example.fromlist was failing\n",
        "valid = valid.reset_index(drop=True) "
      ],
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zp5-PJM48Gm-",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 402
        },
        "outputId": "b140e2a5-cc2b-4f90-9cc9-a60b7a934409"
      },
      "source": [
        "train"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>English</th>\n",
              "      <th>Python</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>46 define a function which can generate a list...</td>\n",
              "      <td>import requests\\ndef get_encoding(url):\\n    d...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>write a function that will provide the ascii v...</td>\n",
              "      <td>def charToASCII(chr):\\n  return f'ASCII value ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>given a python list turn every item of a list ...</td>\n",
              "      <td>aList = [1, 2, 3, 4, 5, 6, 7]\\naList =  [x * x...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>write a function that returns relu value of th...</td>\n",
              "      <td>def relu(x:float) -&gt; float:\\n    x = 0 if x &lt; ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>write a python function to get the surfacearea...</td>\n",
              "      <td>def pyramid_surface_area(base_area, height):\\n...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4627</th>\n",
              "      <td>write a program to move numbers to the end of ...</td>\n",
              "      <td>str1 = 'hi 123 how are you doing? 567 is with ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4628</th>\n",
              "      <td>write a program to strips every vowel from a s...</td>\n",
              "      <td>vowels = ('a', 'e', 'i', 'o', 'u')\\ninput_stri...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4629</th>\n",
              "      <td>6 python add all values of another list</td>\n",
              "      <td>a = [1, 2, 3]\\nb = [4, 5, 6]\\na += b\\n</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4630</th>\n",
              "      <td>calculate the sum of three given numbers if th...</td>\n",
              "      <td>def sum_thrice(x, y, z):\\n    sum1 = x + y + z...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4631</th>\n",
              "      <td>write a python function to check the strength ...</td>\n",
              "      <td>def check_password_strength(password):\\n  impo...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>4632 rows × 2 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "                                                English                                             Python\n",
              "0     46 define a function which can generate a list...  import requests\\ndef get_encoding(url):\\n    d...\n",
              "1     write a function that will provide the ascii v...  def charToASCII(chr):\\n  return f'ASCII value ...\n",
              "2     given a python list turn every item of a list ...  aList = [1, 2, 3, 4, 5, 6, 7]\\naList =  [x * x...\n",
              "3     write a function that returns relu value of th...  def relu(x:float) -> float:\\n    x = 0 if x < ...\n",
              "4     write a python function to get the surfacearea...  def pyramid_surface_area(base_area, height):\\n...\n",
              "...                                                 ...                                                ...\n",
              "4627  write a program to move numbers to the end of ...  str1 = 'hi 123 how are you doing? 567 is with ...\n",
              "4628  write a program to strips every vowel from a s...  vowels = ('a', 'e', 'i', 'o', 'u')\\ninput_stri...\n",
              "4629            6 python add all values of another list             a = [1, 2, 3]\\nb = [4, 5, 6]\\na += b\\n\n",
              "4630  calculate the sum of three given numbers if th...  def sum_thrice(x, y, z):\\n    sum1 = x + y + z...\n",
              "4631  write a python function to check the strength ...  def check_password_strength(password):\\n  impo...\n",
              "\n",
              "[4632 rows x 2 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XxKHMVoiKa4x"
      },
      "source": [
        "MAX_OUTPUT_SEQ_LENGTH = 100"
      ],
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PbqQ8OZCeg_R",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8b214d75-909f-40a9-8f79-e6e6844cbc5e"
      },
      "source": [
        "example_trng = [Example.fromlist([train.English[i],train.Python[i]], fields) for i in range(train.shape[0]) if len(tokenize_python(train.Python[i])) <= MAX_OUTPUT_SEQ_LENGTH - 4 ] \n",
        "example_val = [Example.fromlist([valid.English[i],valid.Python[i]], fields) for i in range(valid.shape[0]) if len(tokenize_python(valid.Python[i])) <= MAX_OUTPUT_SEQ_LENGTH - 4 ] "
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l_-_R27Vfcf2"
      },
      "source": [
        "train_dataset = Dataset(example_trng, fields)\n",
        "valid_dataset = Dataset(example_val, fields)"
      ],
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "urgbsj1pfrIa",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6b828c54-ed97-46d5-f8ee-d77e17a05cba"
      },
      "source": [
        "vars(train_dataset.examples[10])"
      ],
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'English': ['write',\n",
              "  'a',\n",
              "  'python',\n",
              "  'function',\n",
              "  'to',\n",
              "  'read',\n",
              "  'a',\n",
              "  'csv',\n",
              "  'file',\n",
              "  'and',\n",
              "  'print',\n",
              "  'its',\n",
              "  'content'],\n",
              " 'Python': ['def',\n",
              "  'read_csv',\n",
              "  '(',\n",
              "  'filename',\n",
              "  ')',\n",
              "  ':',\n",
              "  'NEWLINE',\n",
              "  'INDENT',\n",
              "  'import',\n",
              "  'csv',\n",
              "  'NEWLINE',\n",
              "  'with',\n",
              "  'open',\n",
              "  '(',\n",
              "  'filename',\n",
              "  ',',\n",
              "  'newline',\n",
              "  '=',\n",
              "  \"''\",\n",
              "  ')',\n",
              "  'as',\n",
              "  'f',\n",
              "  ':',\n",
              "  'NEWLINE',\n",
              "  'INDENT',\n",
              "  'reader',\n",
              "  '=',\n",
              "  'csv',\n",
              "  '.',\n",
              "  'reader',\n",
              "  '(',\n",
              "  'f',\n",
              "  ')',\n",
              "  'NEWLINE',\n",
              "  'for',\n",
              "  'row',\n",
              "  'in',\n",
              "  'reader',\n",
              "  ':',\n",
              "  'NEWLINE',\n",
              "  'INDENT',\n",
              "  'print',\n",
              "  '(',\n",
              "  'row',\n",
              "  ')',\n",
              "  'NEWLINE',\n",
              "  'DEDENT',\n",
              "  'DEDENT',\n",
              "  'DEDENT']}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 29
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eKJnJ1c2yunA",
        "outputId": "9a0cce03-6e4b-4417-a7cc-1eee65fc7883"
      },
      "source": [
        "vars(valid_dataset.examples[10])"
      ],
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'English': ['write',\n",
              "  'a',\n",
              "  'python',\n",
              "  'program',\n",
              "  'to',\n",
              "  'get',\n",
              "  'numbers',\n",
              "  'divisible',\n",
              "  'by',\n",
              "  'fifteen',\n",
              "  'from',\n",
              "  'a',\n",
              "  'list'],\n",
              " 'Python': ['num_list',\n",
              "  '=',\n",
              "  '[',\n",
              "  '45',\n",
              "  ',',\n",
              "  '55',\n",
              "  ',',\n",
              "  '60',\n",
              "  ',',\n",
              "  '37',\n",
              "  ',',\n",
              "  '100',\n",
              "  ',',\n",
              "  '105',\n",
              "  ',',\n",
              "  '220',\n",
              "  ']',\n",
              "  'NEWLINE',\n",
              "  'result',\n",
              "  '=',\n",
              "  'list',\n",
              "  '(',\n",
              "  'filter',\n",
              "  '(',\n",
              "  'lambda',\n",
              "  'x',\n",
              "  ':',\n",
              "  '(',\n",
              "  'x',\n",
              "  '%',\n",
              "  '15',\n",
              "  '==',\n",
              "  '0',\n",
              "  ')',\n",
              "  ',',\n",
              "  'num_list',\n",
              "  ')',\n",
              "  ')',\n",
              "  'NEWLINE',\n",
              "  'print',\n",
              "  '(',\n",
              "  'f\"Numbers divisible by 15 are {result}\"',\n",
              "  ')',\n",
              "  'NEWLINE']}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 30
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v9dIH0Gai-sy"
      },
      "source": [
        "SRC.build_vocab(train_dataset)\n"
      ],
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cZBc8gLAzJha"
      },
      "source": [
        "TRG.build_vocab(train_dataset)"
      ],
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Pv-YIl7KjnxM"
      },
      "source": [
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
      ],
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eYOaLBK9jo6Q"
      },
      "source": [
        "BATCH_SIZE = 128\n",
        "\n",
        "train_iterator, valid_iterator = BucketIterator.splits(\n",
        "    (train_dataset, valid_dataset), \n",
        "    batch_size = BATCH_SIZE,\n",
        "    sort_key=lambda x:len(x.English),\n",
        "    sort_within_batch = False, \n",
        "    device = device)"
      ],
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i7rbQLYHEzFZ"
      },
      "source": [
        "Create our fields to process our data. This will append the \"start of sentence\" and \"end of sentence\" tokens as well as converting all words to lowercase."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BsycLH0DEzFa"
      },
      "source": [
        "Load our data."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Fen1DHy6EzFb"
      },
      "source": [
        "We'll also print out an example just to double check they're not reversed."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RRDnbA09EzFc"
      },
      "source": [
        "Then create our vocabulary, converting all tokens appearing less than twice into `<unk>` tokens."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F2hiioCLEzFd"
      },
      "source": [
        "Finally, define the `device` and create our iterators."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Jq7BwOO4EzFd"
      },
      "source": [
        "## Building the Seq2Seq Model\n",
        "\n",
        "### Encoder\n",
        "\n",
        "![](https://raw.githubusercontent.com/bentrevett/pytorch-seq2seq/9479fcb532214ad26fd4bda9fcf081a05e1aaf4e/assets/transformer-encoder.png)\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "11CnDhTkEzFd"
      },
      "source": [
        "class Encoder(nn.Module):\n",
        "    def __init__(self, \n",
        "                 input_dim, \n",
        "                 hid_dim, \n",
        "                 n_layers, \n",
        "                 n_heads, \n",
        "                 pf_dim,\n",
        "                 dropout, \n",
        "                 device,\n",
        "                 max_length = 100):\n",
        "        super().__init__()\n",
        "\n",
        "        self.device = device\n",
        "        \n",
        "        self.tok_embedding = nn.Embedding(input_dim, hid_dim)\n",
        "        self.pos_embedding = nn.Embedding(max_length, hid_dim)\n",
        "        \n",
        "        self.layers = nn.ModuleList([EncoderLayer(hid_dim, \n",
        "                                                  n_heads, \n",
        "                                                  pf_dim,\n",
        "                                                  dropout, \n",
        "                                                  device) \n",
        "                                     for _ in range(n_layers)])\n",
        "        \n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "        \n",
        "        self.scale = torch.sqrt(torch.FloatTensor([hid_dim])).to(device)\n",
        "        \n",
        "    def forward(self, src, src_mask):\n",
        "        \n",
        "        #src = [batch size, src len]\n",
        "        #src_mask = [batch size, 1, 1, src len]\n",
        "        \n",
        "        batch_size = src.shape[0]\n",
        "        src_len = src.shape[1]\n",
        "        \n",
        "        pos = torch.arange(0, src_len).unsqueeze(0).repeat(batch_size, 1).to(self.device)\n",
        "        \n",
        "        #pos = [batch size, src len]\n",
        "        \n",
        "        src = self.dropout((self.tok_embedding(src) * self.scale) + self.pos_embedding(pos))\n",
        "        #src = (self.tok_embedding(src) * self.scale) + self.pos_embedding(pos)\n",
        "        \n",
        "        #src = [batch size, src len, hid dim]\n",
        "        \n",
        "        for layer in self.layers:\n",
        "            src = layer(src, src_mask)\n",
        "            \n",
        "        #src = [batch size, src len, hid dim]\n",
        "            \n",
        "        return src"
      ],
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ykp4dmnLrA3Y"
      },
      "source": [
        "class EncoderLayer(nn.Module):\n",
        "    def __init__(self, \n",
        "                 hid_dim, \n",
        "                 n_heads, \n",
        "                 pf_dim,  \n",
        "                 dropout, \n",
        "                 device):\n",
        "        super().__init__()\n",
        "        \n",
        "        self.self_attn_layer_norm = nn.LayerNorm(hid_dim)\n",
        "        self.ff_layer_norm = nn.LayerNorm(hid_dim)\n",
        "        self.self_attention = MultiHeadAttentionLayer(hid_dim, n_heads, dropout, device)\n",
        "        self.positionwise_feedforward = PositionwiseFeedforwardLayer(hid_dim, \n",
        "                                                                     pf_dim, \n",
        "                                                                     dropout)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "        \n",
        "    def forward(self, src, src_mask):\n",
        "        \n",
        "        #src = [batch size, src len, hid dim]\n",
        "        #src_mask = [batch size, 1, 1, src len] \n",
        "                \n",
        "        #self attention\n",
        "        _src, _ = self.self_attention(src, src, src, src_mask)\n",
        "        \n",
        "        #dropout, residual connection and layer norm\n",
        "        src = self.self_attn_layer_norm(src + self.dropout(_src))\n",
        "        \n",
        "        #src = [batch size, src len, hid dim]\n",
        "        \n",
        "        #positionwise feedforward\n",
        "        _src = self.positionwise_feedforward(src)\n",
        "        \n",
        "        #dropout, residual and layer norm\n",
        "        src = self.ff_layer_norm(src + self.dropout(_src))\n",
        "        \n",
        "        #src = [batch size, src len, hid dim]\n",
        "        \n",
        "        return src"
      ],
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SiKjBoHqrNMz"
      },
      "source": [
        "class MultiHeadAttentionLayer(nn.Module):\n",
        "    def __init__(self, hid_dim, n_heads, dropout, device):\n",
        "        super().__init__()\n",
        "        \n",
        "        assert hid_dim % n_heads == 0\n",
        "        \n",
        "        self.hid_dim = hid_dim\n",
        "        self.n_heads = n_heads\n",
        "        self.head_dim = hid_dim // n_heads\n",
        "        \n",
        "        self.fc_q = nn.Linear(hid_dim, hid_dim)\n",
        "        self.fc_k = nn.Linear(hid_dim, hid_dim)\n",
        "        self.fc_v = nn.Linear(hid_dim, hid_dim)\n",
        "        \n",
        "        self.fc_o = nn.Linear(hid_dim, hid_dim)\n",
        "        \n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "        \n",
        "        self.scale = torch.sqrt(torch.FloatTensor([self.head_dim])).to(device)\n",
        "        \n",
        "    def forward(self, query, key, value, mask = None):\n",
        "        \n",
        "        batch_size = query.shape[0]\n",
        "        \n",
        "        #query = [batch size, query len, hid dim]\n",
        "        #key = [batch size, key len, hid dim]\n",
        "        #value = [batch size, value len, hid dim]\n",
        "                \n",
        "        Q = self.fc_q(query)\n",
        "        K = self.fc_k(key)\n",
        "        V = self.fc_v(value)\n",
        "        \n",
        "        #Q = [batch size, query len, hid dim]\n",
        "        #K = [batch size, key len, hid dim]\n",
        "        #V = [batch size, value len, hid dim]\n",
        "                \n",
        "        Q = Q.view(batch_size, -1, self.n_heads, self.head_dim).permute(0, 2, 1, 3)\n",
        "        K = K.view(batch_size, -1, self.n_heads, self.head_dim).permute(0, 2, 1, 3)\n",
        "        V = V.view(batch_size, -1, self.n_heads, self.head_dim).permute(0, 2, 1, 3)\n",
        "        \n",
        "        #Q = [batch size, n heads, query len, head dim]\n",
        "        #K = [batch size, n heads, key len, head dim]\n",
        "        #V = [batch size, n heads, value len, head dim]\n",
        "                \n",
        "        energy = torch.matmul(Q, K.permute(0, 1, 3, 2)) / self.scale\n",
        "        \n",
        "        #energy = [batch size, n heads, query len, key len]\n",
        "        \n",
        "        if mask is not None:\n",
        "            energy = energy.masked_fill(mask == 0, -1e10)\n",
        "        \n",
        "        attention = torch.softmax(energy, dim = -1)\n",
        "                \n",
        "        #attention = [batch size, n heads, query len, key len]\n",
        "                \n",
        "        x = torch.matmul(self.dropout(attention), V)\n",
        "        \n",
        "        #x = [batch size, n heads, query len, head dim]\n",
        "        \n",
        "        x = x.permute(0, 2, 1, 3).contiguous()\n",
        "        \n",
        "        #x = [batch size, query len, n heads, head dim]\n",
        "        \n",
        "        x = x.view(batch_size, -1, self.hid_dim)\n",
        "        \n",
        "        #x = [batch size, query len, hid dim]\n",
        "        \n",
        "        x = self.fc_o(x)\n",
        "        \n",
        "        #x = [batch size, query len, hid dim]\n",
        "        \n",
        "        return x, attention"
      ],
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BBp6e8nlrtCg"
      },
      "source": [
        "class PositionwiseFeedforwardLayer(nn.Module):\n",
        "    def __init__(self, hid_dim, pf_dim, dropout):\n",
        "        super().__init__()\n",
        "        \n",
        "        self.fc_1 = nn.Linear(hid_dim, pf_dim)\n",
        "        self.fc_2 = nn.Linear(pf_dim, hid_dim)\n",
        "        \n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "        \n",
        "    def forward(self, x):\n",
        "        \n",
        "        #x = [batch size, seq len, hid dim]\n",
        "        \n",
        "        x = self.dropout(torch.relu(self.fc_1(x)))\n",
        "        \n",
        "        #x = [batch size, seq len, pf dim]\n",
        "        \n",
        "        x = self.fc_2(x)\n",
        "        \n",
        "        #x = [batch size, seq len, hid dim]\n",
        "        \n",
        "        return x"
      ],
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FhBViJ4-EzFe"
      },
      "source": [
        "## Decoder\n",
        "\n",
        "![](https://raw.githubusercontent.com/bentrevett/pytorch-seq2seq/9479fcb532214ad26fd4bda9fcf081a05e1aaf4e/assets/transformer-decoder.png)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wRA8hkiLEzFh"
      },
      "source": [
        "class Decoder(nn.Module):\n",
        "    def __init__(self, \n",
        "                 output_dim, \n",
        "                 hid_dim, \n",
        "                 n_layers, \n",
        "                 n_heads, \n",
        "                 pf_dim, \n",
        "                 dropout, \n",
        "                 device,\n",
        "                 max_length = MAX_OUTPUT_SEQ_LENGTH):\n",
        "        super().__init__()\n",
        "        \n",
        "        self.device = device\n",
        "        #print(\"Decoder __init__ : output_dim=\", output_dim, \" max_length=\", max_length, \" hid dim=\", hid_dim)\n",
        "        \n",
        "        self.tok_embedding = nn.Embedding(output_dim, hid_dim)\n",
        "        self.pos_embedding = nn.Embedding(max_length, hid_dim)\n",
        "        \n",
        "        self.layers = nn.ModuleList([DecoderLayer(hid_dim, \n",
        "                                                  n_heads, \n",
        "                                                  pf_dim, \n",
        "                                                  dropout, \n",
        "                                                  device)\n",
        "                                     for _ in range(n_layers)])\n",
        "        \n",
        "        self.fc_out = nn.Linear(hid_dim, output_dim)\n",
        "        \n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "        \n",
        "        self.scale = torch.sqrt(torch.FloatTensor([hid_dim])).to(device)\n",
        "        self.max_length = max_length\n",
        "        \n",
        "    def forward(self, trg, enc_src, trg_mask, src_mask):\n",
        "        \n",
        "        #trg = [batch size, trg len]\n",
        "        #enc_src = [batch size, src len, hid dim]\n",
        "        #trg_mask = [batch size, 1, trg len, trg len]\n",
        "        #src_mask = [batch size, 1, 1, src len]\n",
        "                \n",
        "        batch_size = trg.shape[0]\n",
        "        trg_len = trg.shape[1]\n",
        "\n",
        "        #print(\"Decoder.forward: batch_size: \", batch_size, \"trg_len=\", trg_len )\n",
        "        \n",
        "        pos = torch.arange(0, trg_len).unsqueeze(0).repeat(batch_size, 1).to(self.device)\n",
        "        #print(\"Decoder.forward: pos =\", pos)\n",
        "        #print(\"Decoder.forward: trg =\", trg)                    \n",
        "        #pos = [batch size, trg len]\n",
        "        tok_embed = self.tok_embedding(trg)\n",
        "       \n",
        "        #print(\"Decoder.forward: tok_embed =\", tok_embed.shape)\n",
        "        pos_embed = self.pos_embedding(pos)\n",
        "        #print(\"Decoder.forward: pos_embed shape =\", pos_embed.shape)\n",
        "            \n",
        "        #trg = self.dropout((self.tok_embedding(trg) * self.scale) + self.pos_embedding(pos))\n",
        "        trg = self.dropout((tok_embed*self.scale) + pos_embed)\n",
        "        #print(\"Decoder.forward: trg=\", trg)\n",
        "        #trg = (self.tok_embedding(trg) * self.scale) + self.pos_embedding(pos)\n",
        "                \n",
        "        #trg = [batch size, trg len, hid dim]\n",
        "        \n",
        "        #print(\"Decoder.forward: before calling layers\")\n",
        "        for layer in self.layers:\n",
        "            trg, attention = layer(trg, enc_src, trg_mask, src_mask)\n",
        "        #print(\"Decoder.forward: after calling layers: trg=\", trg)\n",
        "        #print(\"Decoder.forward: after calling layers: attention=\", trg)\n",
        "        #trg = [batch size, trg len, hid dim]\n",
        "        #attention = [batch size, n heads, trg len, src len]\n",
        "        \n",
        "        #print(\"Decoder.forward: before calling fc_out\")\n",
        "        output = self.fc_out(trg)\n",
        "        #print(\"Decoder.forward: after calling fc_out: output=\", output)\n",
        "        #output = [batch size, trg len, output dim]\n",
        "            \n",
        "        return output, attention"
      ],
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j-QPOMrRr89N"
      },
      "source": [
        "class DecoderLayer(nn.Module):\n",
        "    def __init__(self, \n",
        "                 hid_dim, \n",
        "                 n_heads, \n",
        "                 pf_dim, \n",
        "                 dropout, \n",
        "                 device):\n",
        "        super().__init__()\n",
        "        \n",
        "        self.self_attn_layer_norm = nn.LayerNorm(hid_dim)\n",
        "        self.enc_attn_layer_norm = nn.LayerNorm(hid_dim)\n",
        "        self.ff_layer_norm = nn.LayerNorm(hid_dim)\n",
        "        self.self_attention = MultiHeadAttentionLayer(hid_dim, n_heads, dropout, device)\n",
        "        self.encoder_attention = MultiHeadAttentionLayer(hid_dim, n_heads, dropout, device)\n",
        "        self.positionwise_feedforward = PositionwiseFeedforwardLayer(hid_dim, \n",
        "                                                                     pf_dim, \n",
        "                                                                     dropout)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "        \n",
        "    def forward(self, trg, enc_src, trg_mask, src_mask):\n",
        "        \n",
        "        #trg = [batch size, trg len, hid dim]\n",
        "        #enc_src = [batch size, src len, hid dim]\n",
        "        #trg_mask = [batch size, 1, trg len, trg len]\n",
        "        #src_mask = [batch size, 1, 1, src len]\n",
        "        \n",
        "        #self attention\n",
        "        _trg, _ = self.self_attention(trg, trg, trg, trg_mask)\n",
        "        \n",
        "        #dropout, residual connection and layer norm\n",
        "        trg = self.self_attn_layer_norm(trg + self.dropout(_trg))\n",
        "            \n",
        "        #trg = [batch size, trg len, hid dim]\n",
        "            \n",
        "        #encoder attention\n",
        "        _trg, attention = self.encoder_attention(trg, enc_src, enc_src, src_mask)\n",
        "        # query, key, value\n",
        "        \n",
        "        #dropout, residual connection and layer norm\n",
        "        trg = self.enc_attn_layer_norm(trg + self.dropout(_trg))\n",
        "                    \n",
        "        #trg = [batch size, trg len, hid dim]\n",
        "        \n",
        "        #positionwise feedforward\n",
        "        _trg = self.positionwise_feedforward(trg)\n",
        "        \n",
        "        #dropout, residual and layer norm\n",
        "        trg = self.ff_layer_norm(trg + self.dropout(_trg))\n",
        "        \n",
        "        #trg = [batch size, trg len, hid dim]\n",
        "        #attention = [batch size, n heads, trg len, src len]\n",
        "        \n",
        "        return trg, attention"
      ],
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tV5zN4gEsfHX"
      },
      "source": [
        "class Seq2Seq(nn.Module):\n",
        "    def __init__(self, \n",
        "                 encoder, \n",
        "                 decoder, \n",
        "                 src_pad_idx, \n",
        "                 trg_pad_idx, \n",
        "                 device):\n",
        "        super().__init__()\n",
        "        \n",
        "        self.encoder = encoder\n",
        "        self.decoder = decoder\n",
        "        self.src_pad_idx = src_pad_idx\n",
        "        self.trg_pad_idx = trg_pad_idx\n",
        "        self.device = device\n",
        "        \n",
        "    def make_src_mask(self, src):\n",
        "        \n",
        "        #src = [batch size, src len]\n",
        "        \n",
        "        src_mask = (src != self.src_pad_idx).unsqueeze(1).unsqueeze(2)\n",
        "\n",
        "        #src_mask = [batch size, 1, 1, src len]\n",
        "\n",
        "        return src_mask\n",
        "    \n",
        "    def make_trg_mask(self, trg):\n",
        "        \n",
        "        #trg = [batch size, trg len]\n",
        "        \n",
        "        trg_pad_mask = (trg != self.trg_pad_idx).unsqueeze(1).unsqueeze(2)\n",
        "        \n",
        "        #trg_pad_mask = [batch size, 1, 1, trg len]\n",
        "        \n",
        "        trg_len = trg.shape[1]\n",
        "        \n",
        "        trg_sub_mask = torch.tril(torch.ones((trg_len, trg_len), device = self.device)).bool()\n",
        "        \n",
        "        #trg_sub_mask = [trg len, trg len]\n",
        "            \n",
        "        trg_mask = trg_pad_mask & trg_sub_mask\n",
        "        \n",
        "        #trg_mask = [batch size, 1, trg len, trg len]\n",
        "        \n",
        "        return trg_mask\n",
        "\n",
        "    def forward(self, src, trg):\n",
        "        \n",
        "        #src = [batch size, src len]\n",
        "        #trg = [batch size, trg len]\n",
        "  \n",
        "        src_mask = self.make_src_mask(src)\n",
        "        trg_mask = self.make_trg_mask(trg)\n",
        "        #src_mask = [batch size, 1, 1, src len]\n",
        "        #trg_mask = [batch size, 1, trg len, trg len]\n",
        "        enc_src = self.encoder(src, src_mask)\n",
        "        \n",
        "        #enc_src = [batch size, src len, hid dim]    \n",
        "        output, attention = self.decoder(trg, enc_src, trg_mask, src_mask)\n",
        "        #output = [batch size, trg len, output dim]\n",
        "        #attention = [batch size, n heads, trg len, src len]\n",
        "        \n",
        "        return output, attention"
      ],
      "execution_count": 41,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wqR66FM2EzFj"
      },
      "source": [
        "## Seq2Seq Model\n",
        "\n",
        "Putting the encoder and decoder together, we get:\n",
        "![](https://raw.githubusercontent.com/bentrevett/pytorch-seq2seq/9479fcb532214ad26fd4bda9fcf081a05e1aaf4e/assets/transformer1.png)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uxVMMPUyEzFk"
      },
      "source": [
        "# Training the Seq2Seq Model\n",
        "\n",
        "The rest of this session is very similar to the previous one. \n",
        "\n",
        "We initialise our encoder, decoder and seq2seq model (placing it on the GPU if we have one). As before, the embedding dimensions and the amount of dropout used can be different between the encoder and the decoder, but the hidden dimensions must remain the same."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WDG6jOSuEzFk"
      },
      "source": [
        "INPUT_DIM = len(SRC.vocab)\n",
        "OUTPUT_DIM = len(TRG.vocab)\n",
        "HID_DIM = 256\n",
        "ENC_EMB_DIM = 256\n",
        "DEC_EMB_DIM = 256\n",
        "ENC_LAYERS = 3\n",
        "DEC_LAYERS = 3\n",
        "ENC_HEADS = 8\n",
        "DEC_HEADS = 8\n",
        "ENC_PF_DIM = 512\n",
        "DEC_PF_DIM = 512\n",
        "ENC_DROPOUT = 0.1\n",
        "DEC_DROPOUT = 0.1\n",
        "\n",
        "enc = Encoder(INPUT_DIM, \n",
        "              HID_DIM, \n",
        "              ENC_LAYERS, \n",
        "              ENC_HEADS, \n",
        "              ENC_PF_DIM, \n",
        "              ENC_DROPOUT, \n",
        "              device)\n",
        "\n",
        "dec = Decoder(OUTPUT_DIM, \n",
        "              HID_DIM, \n",
        "              DEC_LAYERS, \n",
        "              DEC_HEADS, \n",
        "              DEC_PF_DIM, \n",
        "              DEC_DROPOUT, \n",
        "              device)"
      ],
      "execution_count": 42,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BPHUOmUH9Alz",
        "outputId": "35a2abf6-d96a-40cf-ab52-1f1d6bd18c6b"
      },
      "source": [
        "OUTPUT_DIM"
      ],
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "4830"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 43
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ToMo3UfHtbHl"
      },
      "source": [
        "SRC_PAD_IDX = SRC.vocab.stoi[SRC.pad_token]\n",
        "SRC_UNK_IDX = SRC.vocab.stoi[SRC.unk_token]\n",
        "TRG_PAD_IDX = TRG.vocab.stoi[TRG.pad_token]\n",
        "TRG_UNK_IDX = TRG.vocab.stoi[TRG.unk_token]\n",
        "\n",
        "model = Seq2Seq(enc, dec, SRC_PAD_IDX, TRG_PAD_IDX, device).to(device)"
      ],
      "execution_count": 44,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "F0dg6wOScJRo",
        "outputId": "f4f578ae-0afe-4eb2-dc6d-57b36548f4e0"
      },
      "source": [
        "def init_weights(m):\n",
        "    for name, param in m.named_parameters():\n",
        "        nn.init.normal_(param.data, mean=0, std=0.01)\n",
        "        \n",
        "model.apply(init_weights)"
      ],
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Seq2Seq(\n",
              "  (encoder): Encoder(\n",
              "    (tok_embedding): Embedding(1912, 256)\n",
              "    (pos_embedding): Embedding(100, 256)\n",
              "    (layers): ModuleList(\n",
              "      (0): EncoderLayer(\n",
              "        (self_attn_layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
              "        (ff_layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
              "        (self_attention): MultiHeadAttentionLayer(\n",
              "          (fc_q): Linear(in_features=256, out_features=256, bias=True)\n",
              "          (fc_k): Linear(in_features=256, out_features=256, bias=True)\n",
              "          (fc_v): Linear(in_features=256, out_features=256, bias=True)\n",
              "          (fc_o): Linear(in_features=256, out_features=256, bias=True)\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "        (positionwise_feedforward): PositionwiseFeedforwardLayer(\n",
              "          (fc_1): Linear(in_features=256, out_features=512, bias=True)\n",
              "          (fc_2): Linear(in_features=512, out_features=256, bias=True)\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "        (dropout): Dropout(p=0.1, inplace=False)\n",
              "      )\n",
              "      (1): EncoderLayer(\n",
              "        (self_attn_layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
              "        (ff_layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
              "        (self_attention): MultiHeadAttentionLayer(\n",
              "          (fc_q): Linear(in_features=256, out_features=256, bias=True)\n",
              "          (fc_k): Linear(in_features=256, out_features=256, bias=True)\n",
              "          (fc_v): Linear(in_features=256, out_features=256, bias=True)\n",
              "          (fc_o): Linear(in_features=256, out_features=256, bias=True)\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "        (positionwise_feedforward): PositionwiseFeedforwardLayer(\n",
              "          (fc_1): Linear(in_features=256, out_features=512, bias=True)\n",
              "          (fc_2): Linear(in_features=512, out_features=256, bias=True)\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "        (dropout): Dropout(p=0.1, inplace=False)\n",
              "      )\n",
              "      (2): EncoderLayer(\n",
              "        (self_attn_layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
              "        (ff_layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
              "        (self_attention): MultiHeadAttentionLayer(\n",
              "          (fc_q): Linear(in_features=256, out_features=256, bias=True)\n",
              "          (fc_k): Linear(in_features=256, out_features=256, bias=True)\n",
              "          (fc_v): Linear(in_features=256, out_features=256, bias=True)\n",
              "          (fc_o): Linear(in_features=256, out_features=256, bias=True)\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "        (positionwise_feedforward): PositionwiseFeedforwardLayer(\n",
              "          (fc_1): Linear(in_features=256, out_features=512, bias=True)\n",
              "          (fc_2): Linear(in_features=512, out_features=256, bias=True)\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "        (dropout): Dropout(p=0.1, inplace=False)\n",
              "      )\n",
              "    )\n",
              "    (dropout): Dropout(p=0.1, inplace=False)\n",
              "  )\n",
              "  (decoder): Decoder(\n",
              "    (tok_embedding): Embedding(4830, 256)\n",
              "    (pos_embedding): Embedding(100, 256)\n",
              "    (layers): ModuleList(\n",
              "      (0): DecoderLayer(\n",
              "        (self_attn_layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
              "        (enc_attn_layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
              "        (ff_layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
              "        (self_attention): MultiHeadAttentionLayer(\n",
              "          (fc_q): Linear(in_features=256, out_features=256, bias=True)\n",
              "          (fc_k): Linear(in_features=256, out_features=256, bias=True)\n",
              "          (fc_v): Linear(in_features=256, out_features=256, bias=True)\n",
              "          (fc_o): Linear(in_features=256, out_features=256, bias=True)\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "        (encoder_attention): MultiHeadAttentionLayer(\n",
              "          (fc_q): Linear(in_features=256, out_features=256, bias=True)\n",
              "          (fc_k): Linear(in_features=256, out_features=256, bias=True)\n",
              "          (fc_v): Linear(in_features=256, out_features=256, bias=True)\n",
              "          (fc_o): Linear(in_features=256, out_features=256, bias=True)\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "        (positionwise_feedforward): PositionwiseFeedforwardLayer(\n",
              "          (fc_1): Linear(in_features=256, out_features=512, bias=True)\n",
              "          (fc_2): Linear(in_features=512, out_features=256, bias=True)\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "        (dropout): Dropout(p=0.1, inplace=False)\n",
              "      )\n",
              "      (1): DecoderLayer(\n",
              "        (self_attn_layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
              "        (enc_attn_layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
              "        (ff_layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
              "        (self_attention): MultiHeadAttentionLayer(\n",
              "          (fc_q): Linear(in_features=256, out_features=256, bias=True)\n",
              "          (fc_k): Linear(in_features=256, out_features=256, bias=True)\n",
              "          (fc_v): Linear(in_features=256, out_features=256, bias=True)\n",
              "          (fc_o): Linear(in_features=256, out_features=256, bias=True)\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "        (encoder_attention): MultiHeadAttentionLayer(\n",
              "          (fc_q): Linear(in_features=256, out_features=256, bias=True)\n",
              "          (fc_k): Linear(in_features=256, out_features=256, bias=True)\n",
              "          (fc_v): Linear(in_features=256, out_features=256, bias=True)\n",
              "          (fc_o): Linear(in_features=256, out_features=256, bias=True)\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "        (positionwise_feedforward): PositionwiseFeedforwardLayer(\n",
              "          (fc_1): Linear(in_features=256, out_features=512, bias=True)\n",
              "          (fc_2): Linear(in_features=512, out_features=256, bias=True)\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "        (dropout): Dropout(p=0.1, inplace=False)\n",
              "      )\n",
              "      (2): DecoderLayer(\n",
              "        (self_attn_layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
              "        (enc_attn_layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
              "        (ff_layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
              "        (self_attention): MultiHeadAttentionLayer(\n",
              "          (fc_q): Linear(in_features=256, out_features=256, bias=True)\n",
              "          (fc_k): Linear(in_features=256, out_features=256, bias=True)\n",
              "          (fc_v): Linear(in_features=256, out_features=256, bias=True)\n",
              "          (fc_o): Linear(in_features=256, out_features=256, bias=True)\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "        (encoder_attention): MultiHeadAttentionLayer(\n",
              "          (fc_q): Linear(in_features=256, out_features=256, bias=True)\n",
              "          (fc_k): Linear(in_features=256, out_features=256, bias=True)\n",
              "          (fc_v): Linear(in_features=256, out_features=256, bias=True)\n",
              "          (fc_o): Linear(in_features=256, out_features=256, bias=True)\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "        (positionwise_feedforward): PositionwiseFeedforwardLayer(\n",
              "          (fc_1): Linear(in_features=256, out_features=512, bias=True)\n",
              "          (fc_2): Linear(in_features=512, out_features=256, bias=True)\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "        (dropout): Dropout(p=0.1, inplace=False)\n",
              "      )\n",
              "    )\n",
              "    (fc_out): Linear(in_features=256, out_features=4830, bias=True)\n",
              "    (dropout): Dropout(p=0.1, inplace=False)\n",
              "  )\n",
              ")"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 45
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-7OOAgyOZzQg"
      },
      "source": [
        "enc.tok_embedding.weight.data[SRC_PAD_IDX] = torch.zeros(ENC_EMB_DIM)\n",
        "enc.tok_embedding.weight.data[SRC_UNK_IDX] = torch.zeros(ENC_EMB_DIM)"
      ],
      "execution_count": 46,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dElc7zww_UuJ"
      },
      "source": [
        "dec.tok_embedding.weight.data[TRG_PAD_IDX] = torch.zeros(DEC_EMB_DIM)\n",
        "dec.tok_embedding.weight.data[SRC_UNK_IDX] = torch.zeros(DEC_EMB_DIM)"
      ],
      "execution_count": 47,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RLIc1l6CEzFk"
      },
      "source": [
        "Next, we initialize our parameters. The paper states the parameters are initialized from a normal distribution with a mean of 0 and a standard deviation of 0.01, i.e. $\\mathcal{N}(0, 0.01)$. \n",
        "\n",
        "It also states we should initialize the recurrent parameters to a special initialization, however to keep things simple we'll also initialize them to $\\mathcal{N}(0, 0.01)$."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hd1QoOsUEzFl"
      },
      "source": [
        "We print out the number of parameters.\n",
        "\n",
        "Even though we only have a single layer RNN for our encoder and decoder we actually have **more** parameters  than the last model. This is due to the increased size of the inputs to the GRU and the linear layer. However, it is not a significant amount of parameters and causes a minimal amount of increase in training time (~3 seconds per epoch extra)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IggCwIBgEzFl",
        "outputId": "7f63ae62-56a9-400c-8d8e-c8b136d45516"
      },
      "source": [
        "def count_parameters(model):\n",
        "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "\n",
        "print(f'The model has {count_parameters(model):,} trainable parameters')"
      ],
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "The model has 6,972,126 trainable parameters\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YiH54qzHEzFl"
      },
      "source": [
        "We initiaize our optimizer."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eO1_eoG7EzFl"
      },
      "source": [
        "LEARNING_RATE = 0.0005\n",
        "\n",
        "\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr = LEARNING_RATE)"
      ],
      "execution_count": 49,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BwkV3FKlEzFl"
      },
      "source": [
        "We also initialize the loss function, making sure to ignore the loss on `<pad>` tokens."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H0DAbGbcEzFl"
      },
      "source": [
        "criterion = nn.CrossEntropyLoss(ignore_index = TRG_PAD_IDX)\n"
      ],
      "execution_count": 50,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FTay7E9rEzFm"
      },
      "source": [
        "We then create the training loop..."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fCw7utNlucv5"
      },
      "source": [
        "def train(model, iterator, optimizer, criterion, clip):\n",
        "    model.train()\n",
        "    \n",
        "    epoch_loss = 0\n",
        "    \n",
        "    for i, batch in enumerate(iterator):\n",
        "        \n",
        "        src = batch.English\n",
        "        trg = batch.Python\n",
        "        \n",
        "        optimizer.zero_grad()\n",
        "        \n",
        "        output, _ = model(src, trg[:,:-1])\n",
        "                \n",
        "        #output = [batch size, trg len - 1, output dim]\n",
        "        #trg = [batch size, trg len]\n",
        "            \n",
        "        output_dim = output.shape[-1]\n",
        "            \n",
        "        output = output.contiguous().view(-1, output_dim)\n",
        "        trg = trg[:,1:].contiguous().view(-1)\n",
        "                \n",
        "        #output = [batch size * trg len - 1, output dim]\n",
        "        #trg = [batch size * trg len - 1]\n",
        "            \n",
        "        loss = criterion(output, trg)\n",
        "        \n",
        "        loss.backward()\n",
        "        \n",
        "        torch.nn.utils.clip_grad_norm_(model.parameters(), clip)\n",
        "        \n",
        "        optimizer.step()\n",
        "        \n",
        "        epoch_loss += loss.item()\n",
        "        \n",
        "    return epoch_loss / len(iterator)"
      ],
      "execution_count": 51,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "McZu2YzRNKxW"
      },
      "source": [
        "def evaluate(model, iterator, criterion):\n",
        "    \n",
        "    model.eval()\n",
        "    \n",
        "    epoch_loss = 0\n",
        "    \n",
        "    with torch.no_grad():\n",
        "    \n",
        "        for i, batch in enumerate(iterator):\n",
        "\n",
        "            src = batch.English\n",
        "            trg = batch.Python\n",
        "\n",
        "            output, _ = model(src, trg[:,:-1])\n",
        "            \n",
        "            #output = [batch size, trg len - 1, output dim]\n",
        "            #trg = [batch size, trg len]\n",
        "            \n",
        "            output_dim = output.shape[-1]\n",
        "            \n",
        "            output = output.contiguous().view(-1, output_dim)\n",
        "            trg = trg[:,1:].contiguous().view(-1)\n",
        "            \n",
        "            #output = [batch size * trg len - 1, output dim]\n",
        "            #trg = [batch size * trg len - 1]\n",
        "            \n",
        "            loss = criterion(output, trg)\n",
        "\n",
        "            epoch_loss += loss.item()\n",
        "        \n",
        "    return epoch_loss / len(iterator)"
      ],
      "execution_count": 76,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6rfUx5lhEzFm"
      },
      "source": [
        "...and the evaluation loop, remembering to set the model to `eval` mode and turn off teaching forcing."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ODZTe6_J1mXz"
      },
      "source": [
        "def epoch_time(start_time, end_time):\n",
        "    elapsed_time = end_time - start_time\n",
        "    elapsed_mins = int(elapsed_time / 60)\n",
        "    elapsed_secs = int(elapsed_time - (elapsed_mins * 60))\n",
        "    return elapsed_mins, elapsed_secs\n"
      ],
      "execution_count": 77,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SZUlYPHQnePo"
      },
      "source": [
        ""
      ],
      "execution_count": 77,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Jw022pw0EzFm",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2d3041f5-75d3-4173-d120-24db84699d53"
      },
      "source": [
        "N_EPOCHS = 250\n",
        "CLIP = 1\n",
        "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
        "\n",
        "scheduler = ReduceLROnPlateau(optimizer, mode='min', factor=0.8, patience=10, verbose=True)\n",
        "\n",
        "best_valid_loss = float('inf')\n",
        "\n",
        "for epoch in range(N_EPOCHS):\n",
        "    \n",
        "    start_time = time.time()\n",
        "    \n",
        "    train_loss = train(model, train_iterator, optimizer, criterion, CLIP)\n",
        "    valid_loss = evaluate(model, valid_iterator, criterion)\n",
        "    \n",
        "    end_time = time.time()\n",
        "    \n",
        "    epoch_mins, epoch_secs = epoch_time(start_time, end_time)\n",
        "    \n",
        "    if valid_loss < best_valid_loss:\n",
        "        best_valid_loss = valid_loss\n",
        "        torch.save(model.state_dict(), 'english_python.pt')\n",
        "    scheduler.step(train_loss)\n",
        "    print(f'Epoch: {epoch+1:02} | Time: {epoch_mins}m {epoch_secs}s')\n",
        "    print(f'\\tTrain Loss: {train_loss:.3f} | Train PPL: {math.exp(train_loss):7.3f}')\n",
        "    print(f'\\t Val. Loss: {valid_loss:.3f} |  Val. PPL: {math.exp(valid_loss):7.3f}')"
      ],
      "execution_count": 78,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch: 01 | Time: 0m 4s\n",
            "\tTrain Loss: 4.901 | Train PPL: 134.465\n",
            "\t Val. Loss: 4.195 |  Val. PPL:  66.358\n",
            "Epoch: 02 | Time: 0m 4s\n",
            "\tTrain Loss: 3.731 | Train PPL:  41.722\n",
            "\t Val. Loss: 3.297 |  Val. PPL:  27.024\n",
            "Epoch: 03 | Time: 0m 4s\n",
            "\tTrain Loss: 3.009 | Train PPL:  20.259\n",
            "\t Val. Loss: 2.760 |  Val. PPL:  15.797\n",
            "Epoch: 04 | Time: 0m 4s\n",
            "\tTrain Loss: 2.565 | Train PPL:  13.003\n",
            "\t Val. Loss: 2.435 |  Val. PPL:  11.417\n",
            "Epoch: 05 | Time: 0m 4s\n",
            "\tTrain Loss: 2.277 | Train PPL:   9.745\n",
            "\t Val. Loss: 2.211 |  Val. PPL:   9.123\n",
            "Epoch: 06 | Time: 0m 4s\n",
            "\tTrain Loss: 2.070 | Train PPL:   7.927\n",
            "\t Val. Loss: 2.059 |  Val. PPL:   7.838\n",
            "Epoch: 07 | Time: 0m 4s\n",
            "\tTrain Loss: 1.910 | Train PPL:   6.754\n",
            "\t Val. Loss: 1.929 |  Val. PPL:   6.884\n",
            "Epoch: 08 | Time: 0m 4s\n",
            "\tTrain Loss: 1.781 | Train PPL:   5.937\n",
            "\t Val. Loss: 1.814 |  Val. PPL:   6.136\n",
            "Epoch: 09 | Time: 0m 4s\n",
            "\tTrain Loss: 1.664 | Train PPL:   5.282\n",
            "\t Val. Loss: 1.721 |  Val. PPL:   5.592\n",
            "Epoch: 10 | Time: 0m 4s\n",
            "\tTrain Loss: 1.566 | Train PPL:   4.787\n",
            "\t Val. Loss: 1.629 |  Val. PPL:   5.099\n",
            "Epoch: 11 | Time: 0m 4s\n",
            "\tTrain Loss: 1.476 | Train PPL:   4.378\n",
            "\t Val. Loss: 1.552 |  Val. PPL:   4.722\n",
            "Epoch: 12 | Time: 0m 4s\n",
            "\tTrain Loss: 1.395 | Train PPL:   4.036\n",
            "\t Val. Loss: 1.490 |  Val. PPL:   4.436\n",
            "Epoch: 13 | Time: 0m 4s\n",
            "\tTrain Loss: 1.316 | Train PPL:   3.728\n",
            "\t Val. Loss: 1.424 |  Val. PPL:   4.153\n",
            "Epoch: 14 | Time: 0m 4s\n",
            "\tTrain Loss: 1.245 | Train PPL:   3.473\n",
            "\t Val. Loss: 1.376 |  Val. PPL:   3.957\n",
            "Epoch: 15 | Time: 0m 4s\n",
            "\tTrain Loss: 1.181 | Train PPL:   3.256\n",
            "\t Val. Loss: 1.316 |  Val. PPL:   3.729\n",
            "Epoch: 16 | Time: 0m 4s\n",
            "\tTrain Loss: 1.124 | Train PPL:   3.077\n",
            "\t Val. Loss: 1.266 |  Val. PPL:   3.545\n",
            "Epoch: 17 | Time: 0m 4s\n",
            "\tTrain Loss: 1.067 | Train PPL:   2.908\n",
            "\t Val. Loss: 1.220 |  Val. PPL:   3.386\n",
            "Epoch: 18 | Time: 0m 4s\n",
            "\tTrain Loss: 1.015 | Train PPL:   2.760\n",
            "\t Val. Loss: 1.186 |  Val. PPL:   3.272\n",
            "Epoch: 19 | Time: 0m 4s\n",
            "\tTrain Loss: 0.965 | Train PPL:   2.624\n",
            "\t Val. Loss: 1.139 |  Val. PPL:   3.123\n",
            "Epoch: 20 | Time: 0m 4s\n",
            "\tTrain Loss: 0.920 | Train PPL:   2.508\n",
            "\t Val. Loss: 1.103 |  Val. PPL:   3.014\n",
            "Epoch: 21 | Time: 0m 4s\n",
            "\tTrain Loss: 0.876 | Train PPL:   2.401\n",
            "\t Val. Loss: 1.052 |  Val. PPL:   2.862\n",
            "Epoch: 22 | Time: 0m 4s\n",
            "\tTrain Loss: 0.829 | Train PPL:   2.292\n",
            "\t Val. Loss: 1.024 |  Val. PPL:   2.784\n",
            "Epoch: 23 | Time: 0m 4s\n",
            "\tTrain Loss: 0.795 | Train PPL:   2.213\n",
            "\t Val. Loss: 0.990 |  Val. PPL:   2.692\n",
            "Epoch: 24 | Time: 0m 4s\n",
            "\tTrain Loss: 0.762 | Train PPL:   2.142\n",
            "\t Val. Loss: 0.971 |  Val. PPL:   2.639\n",
            "Epoch: 25 | Time: 0m 4s\n",
            "\tTrain Loss: 0.724 | Train PPL:   2.063\n",
            "\t Val. Loss: 0.944 |  Val. PPL:   2.569\n",
            "Epoch: 26 | Time: 0m 4s\n",
            "\tTrain Loss: 0.692 | Train PPL:   1.998\n",
            "\t Val. Loss: 0.904 |  Val. PPL:   2.468\n",
            "Epoch: 27 | Time: 0m 4s\n",
            "\tTrain Loss: 0.659 | Train PPL:   1.933\n",
            "\t Val. Loss: 0.874 |  Val. PPL:   2.395\n",
            "Epoch: 28 | Time: 0m 4s\n",
            "\tTrain Loss: 0.629 | Train PPL:   1.877\n",
            "\t Val. Loss: 0.858 |  Val. PPL:   2.358\n",
            "Epoch: 29 | Time: 0m 4s\n",
            "\tTrain Loss: 0.598 | Train PPL:   1.818\n",
            "\t Val. Loss: 0.830 |  Val. PPL:   2.292\n",
            "Epoch: 30 | Time: 0m 4s\n",
            "\tTrain Loss: 0.571 | Train PPL:   1.770\n",
            "\t Val. Loss: 0.805 |  Val. PPL:   2.236\n",
            "Epoch: 31 | Time: 0m 4s\n",
            "\tTrain Loss: 0.548 | Train PPL:   1.729\n",
            "\t Val. Loss: 0.782 |  Val. PPL:   2.186\n",
            "Epoch: 32 | Time: 0m 4s\n",
            "\tTrain Loss: 0.521 | Train PPL:   1.684\n",
            "\t Val. Loss: 0.767 |  Val. PPL:   2.153\n",
            "Epoch: 33 | Time: 0m 4s\n",
            "\tTrain Loss: 0.498 | Train PPL:   1.645\n",
            "\t Val. Loss: 0.746 |  Val. PPL:   2.109\n",
            "Epoch: 34 | Time: 0m 4s\n",
            "\tTrain Loss: 0.474 | Train PPL:   1.606\n",
            "\t Val. Loss: 0.729 |  Val. PPL:   2.072\n",
            "Epoch: 35 | Time: 0m 4s\n",
            "\tTrain Loss: 0.449 | Train PPL:   1.567\n",
            "\t Val. Loss: 0.714 |  Val. PPL:   2.042\n",
            "Epoch: 36 | Time: 0m 4s\n",
            "\tTrain Loss: 0.430 | Train PPL:   1.537\n",
            "\t Val. Loss: 0.693 |  Val. PPL:   2.000\n",
            "Epoch: 37 | Time: 0m 4s\n",
            "\tTrain Loss: 0.411 | Train PPL:   1.508\n",
            "\t Val. Loss: 0.677 |  Val. PPL:   1.969\n",
            "Epoch: 38 | Time: 0m 4s\n",
            "\tTrain Loss: 0.392 | Train PPL:   1.480\n",
            "\t Val. Loss: 0.657 |  Val. PPL:   1.930\n",
            "Epoch: 39 | Time: 0m 4s\n",
            "\tTrain Loss: 0.378 | Train PPL:   1.459\n",
            "\t Val. Loss: 0.645 |  Val. PPL:   1.905\n",
            "Epoch: 40 | Time: 0m 4s\n",
            "\tTrain Loss: 0.364 | Train PPL:   1.439\n",
            "\t Val. Loss: 0.621 |  Val. PPL:   1.860\n",
            "Epoch: 41 | Time: 0m 4s\n",
            "\tTrain Loss: 0.343 | Train PPL:   1.410\n",
            "\t Val. Loss: 0.625 |  Val. PPL:   1.868\n",
            "Epoch: 42 | Time: 0m 4s\n",
            "\tTrain Loss: 0.326 | Train PPL:   1.385\n",
            "\t Val. Loss: 0.605 |  Val. PPL:   1.831\n",
            "Epoch: 43 | Time: 0m 4s\n",
            "\tTrain Loss: 0.312 | Train PPL:   1.366\n",
            "\t Val. Loss: 0.595 |  Val. PPL:   1.812\n",
            "Epoch: 44 | Time: 0m 4s\n",
            "\tTrain Loss: 0.303 | Train PPL:   1.354\n",
            "\t Val. Loss: 0.592 |  Val. PPL:   1.808\n",
            "Epoch: 45 | Time: 0m 4s\n",
            "\tTrain Loss: 0.291 | Train PPL:   1.337\n",
            "\t Val. Loss: 0.581 |  Val. PPL:   1.788\n",
            "Epoch: 46 | Time: 0m 4s\n",
            "\tTrain Loss: 0.276 | Train PPL:   1.318\n",
            "\t Val. Loss: 0.576 |  Val. PPL:   1.779\n",
            "Epoch: 47 | Time: 0m 4s\n",
            "\tTrain Loss: 0.266 | Train PPL:   1.305\n",
            "\t Val. Loss: 0.562 |  Val. PPL:   1.755\n",
            "Epoch: 48 | Time: 0m 4s\n",
            "\tTrain Loss: 0.257 | Train PPL:   1.293\n",
            "\t Val. Loss: 0.551 |  Val. PPL:   1.736\n",
            "Epoch: 49 | Time: 0m 4s\n",
            "\tTrain Loss: 0.245 | Train PPL:   1.277\n",
            "\t Val. Loss: 0.543 |  Val. PPL:   1.722\n",
            "Epoch: 50 | Time: 0m 4s\n",
            "\tTrain Loss: 0.239 | Train PPL:   1.270\n",
            "\t Val. Loss: 0.549 |  Val. PPL:   1.732\n",
            "Epoch: 51 | Time: 0m 4s\n",
            "\tTrain Loss: 0.226 | Train PPL:   1.254\n",
            "\t Val. Loss: 0.538 |  Val. PPL:   1.712\n",
            "Epoch: 52 | Time: 0m 4s\n",
            "\tTrain Loss: 0.220 | Train PPL:   1.247\n",
            "\t Val. Loss: 0.528 |  Val. PPL:   1.696\n",
            "Epoch: 53 | Time: 0m 4s\n",
            "\tTrain Loss: 0.213 | Train PPL:   1.237\n",
            "\t Val. Loss: 0.532 |  Val. PPL:   1.703\n",
            "Epoch: 54 | Time: 0m 4s\n",
            "\tTrain Loss: 0.204 | Train PPL:   1.227\n",
            "\t Val. Loss: 0.521 |  Val. PPL:   1.684\n",
            "Epoch: 55 | Time: 0m 4s\n",
            "\tTrain Loss: 0.196 | Train PPL:   1.216\n",
            "\t Val. Loss: 0.531 |  Val. PPL:   1.700\n",
            "Epoch: 56 | Time: 0m 4s\n",
            "\tTrain Loss: 0.191 | Train PPL:   1.210\n",
            "\t Val. Loss: 0.509 |  Val. PPL:   1.664\n",
            "Epoch: 57 | Time: 0m 4s\n",
            "\tTrain Loss: 0.183 | Train PPL:   1.201\n",
            "\t Val. Loss: 0.529 |  Val. PPL:   1.698\n",
            "Epoch: 58 | Time: 0m 4s\n",
            "\tTrain Loss: 0.178 | Train PPL:   1.195\n",
            "\t Val. Loss: 0.506 |  Val. PPL:   1.658\n",
            "Epoch: 59 | Time: 0m 4s\n",
            "\tTrain Loss: 0.170 | Train PPL:   1.185\n",
            "\t Val. Loss: 0.508 |  Val. PPL:   1.661\n",
            "Epoch: 60 | Time: 0m 4s\n",
            "\tTrain Loss: 0.168 | Train PPL:   1.183\n",
            "\t Val. Loss: 0.496 |  Val. PPL:   1.642\n",
            "Epoch: 61 | Time: 0m 4s\n",
            "\tTrain Loss: 0.163 | Train PPL:   1.177\n",
            "\t Val. Loss: 0.513 |  Val. PPL:   1.670\n",
            "Epoch: 62 | Time: 0m 4s\n",
            "\tTrain Loss: 0.157 | Train PPL:   1.170\n",
            "\t Val. Loss: 0.510 |  Val. PPL:   1.666\n",
            "Epoch: 63 | Time: 0m 4s\n",
            "\tTrain Loss: 0.153 | Train PPL:   1.165\n",
            "\t Val. Loss: 0.503 |  Val. PPL:   1.653\n",
            "Epoch: 64 | Time: 0m 4s\n",
            "\tTrain Loss: 0.150 | Train PPL:   1.162\n",
            "\t Val. Loss: 0.502 |  Val. PPL:   1.653\n",
            "Epoch: 65 | Time: 0m 4s\n",
            "\tTrain Loss: 0.145 | Train PPL:   1.156\n",
            "\t Val. Loss: 0.501 |  Val. PPL:   1.650\n",
            "Epoch: 66 | Time: 0m 4s\n",
            "\tTrain Loss: 0.141 | Train PPL:   1.151\n",
            "\t Val. Loss: 0.517 |  Val. PPL:   1.678\n",
            "Epoch: 67 | Time: 0m 4s\n",
            "\tTrain Loss: 0.138 | Train PPL:   1.148\n",
            "\t Val. Loss: 0.519 |  Val. PPL:   1.680\n",
            "Epoch: 68 | Time: 0m 4s\n",
            "\tTrain Loss: 0.136 | Train PPL:   1.145\n",
            "\t Val. Loss: 0.512 |  Val. PPL:   1.669\n",
            "Epoch: 69 | Time: 0m 4s\n",
            "\tTrain Loss: 0.135 | Train PPL:   1.145\n",
            "\t Val. Loss: 0.499 |  Val. PPL:   1.646\n",
            "Epoch: 70 | Time: 0m 4s\n",
            "\tTrain Loss: 0.128 | Train PPL:   1.137\n",
            "\t Val. Loss: 0.502 |  Val. PPL:   1.652\n",
            "Epoch: 71 | Time: 0m 4s\n",
            "\tTrain Loss: 0.126 | Train PPL:   1.134\n",
            "\t Val. Loss: 0.526 |  Val. PPL:   1.691\n",
            "Epoch: 72 | Time: 0m 4s\n",
            "\tTrain Loss: 0.123 | Train PPL:   1.131\n",
            "\t Val. Loss: 0.498 |  Val. PPL:   1.645\n",
            "Epoch: 73 | Time: 0m 4s\n",
            "\tTrain Loss: 0.121 | Train PPL:   1.129\n",
            "\t Val. Loss: 0.485 |  Val. PPL:   1.624\n",
            "Epoch: 74 | Time: 0m 4s\n",
            "\tTrain Loss: 0.117 | Train PPL:   1.124\n",
            "\t Val. Loss: 0.513 |  Val. PPL:   1.671\n",
            "Epoch: 75 | Time: 0m 4s\n",
            "\tTrain Loss: 0.116 | Train PPL:   1.122\n",
            "\t Val. Loss: 0.502 |  Val. PPL:   1.652\n",
            "Epoch: 76 | Time: 0m 4s\n",
            "\tTrain Loss: 0.112 | Train PPL:   1.119\n",
            "\t Val. Loss: 0.511 |  Val. PPL:   1.666\n",
            "Epoch: 77 | Time: 0m 4s\n",
            "\tTrain Loss: 0.110 | Train PPL:   1.116\n",
            "\t Val. Loss: 0.513 |  Val. PPL:   1.670\n",
            "Epoch: 78 | Time: 0m 4s\n",
            "\tTrain Loss: 0.108 | Train PPL:   1.114\n",
            "\t Val. Loss: 0.514 |  Val. PPL:   1.672\n",
            "Epoch: 79 | Time: 0m 4s\n",
            "\tTrain Loss: 0.104 | Train PPL:   1.109\n",
            "\t Val. Loss: 0.503 |  Val. PPL:   1.654\n",
            "Epoch: 80 | Time: 0m 4s\n",
            "\tTrain Loss: 0.105 | Train PPL:   1.111\n",
            "\t Val. Loss: 0.485 |  Val. PPL:   1.624\n",
            "Epoch: 81 | Time: 0m 4s\n",
            "\tTrain Loss: 0.102 | Train PPL:   1.107\n",
            "\t Val. Loss: 0.512 |  Val. PPL:   1.668\n",
            "Epoch: 82 | Time: 0m 4s\n",
            "\tTrain Loss: 0.100 | Train PPL:   1.105\n",
            "\t Val. Loss: 0.519 |  Val. PPL:   1.681\n",
            "Epoch: 83 | Time: 0m 4s\n",
            "\tTrain Loss: 0.098 | Train PPL:   1.103\n",
            "\t Val. Loss: 0.505 |  Val. PPL:   1.657\n",
            "Epoch: 84 | Time: 0m 4s\n",
            "\tTrain Loss: 0.098 | Train PPL:   1.103\n",
            "\t Val. Loss: 0.497 |  Val. PPL:   1.644\n",
            "Epoch: 85 | Time: 0m 4s\n",
            "\tTrain Loss: 0.095 | Train PPL:   1.099\n",
            "\t Val. Loss: 0.502 |  Val. PPL:   1.651\n",
            "Epoch: 86 | Time: 0m 4s\n",
            "\tTrain Loss: 0.097 | Train PPL:   1.102\n",
            "\t Val. Loss: 0.499 |  Val. PPL:   1.647\n",
            "Epoch: 87 | Time: 0m 4s\n",
            "\tTrain Loss: 0.092 | Train PPL:   1.096\n",
            "\t Val. Loss: 0.501 |  Val. PPL:   1.651\n",
            "Epoch: 88 | Time: 0m 4s\n",
            "\tTrain Loss: 0.090 | Train PPL:   1.094\n",
            "\t Val. Loss: 0.509 |  Val. PPL:   1.664\n",
            "Epoch: 89 | Time: 0m 4s\n",
            "\tTrain Loss: 0.088 | Train PPL:   1.092\n",
            "\t Val. Loss: 0.513 |  Val. PPL:   1.670\n",
            "Epoch: 90 | Time: 0m 4s\n",
            "\tTrain Loss: 0.088 | Train PPL:   1.092\n",
            "\t Val. Loss: 0.500 |  Val. PPL:   1.648\n",
            "Epoch: 91 | Time: 0m 4s\n",
            "\tTrain Loss: 0.085 | Train PPL:   1.089\n",
            "\t Val. Loss: 0.503 |  Val. PPL:   1.654\n",
            "Epoch: 92 | Time: 0m 4s\n",
            "\tTrain Loss: 0.085 | Train PPL:   1.088\n",
            "\t Val. Loss: 0.516 |  Val. PPL:   1.676\n",
            "Epoch: 93 | Time: 0m 4s\n",
            "\tTrain Loss: 0.084 | Train PPL:   1.087\n",
            "\t Val. Loss: 0.517 |  Val. PPL:   1.677\n",
            "Epoch: 94 | Time: 0m 4s\n",
            "\tTrain Loss: 0.081 | Train PPL:   1.084\n",
            "\t Val. Loss: 0.523 |  Val. PPL:   1.687\n",
            "Epoch: 95 | Time: 0m 4s\n",
            "\tTrain Loss: 0.082 | Train PPL:   1.085\n",
            "\t Val. Loss: 0.523 |  Val. PPL:   1.688\n",
            "Epoch: 96 | Time: 0m 4s\n",
            "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
            "\t Val. Loss: 0.524 |  Val. PPL:   1.689\n",
            "Epoch: 97 | Time: 0m 4s\n",
            "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
            "\t Val. Loss: 0.530 |  Val. PPL:   1.699\n",
            "Epoch: 98 | Time: 0m 4s\n",
            "\tTrain Loss: 0.076 | Train PPL:   1.079\n",
            "\t Val. Loss: 0.521 |  Val. PPL:   1.684\n",
            "Epoch: 99 | Time: 0m 4s\n",
            "\tTrain Loss: 0.077 | Train PPL:   1.081\n",
            "\t Val. Loss: 0.523 |  Val. PPL:   1.688\n",
            "Epoch: 100 | Time: 0m 4s\n",
            "\tTrain Loss: 0.076 | Train PPL:   1.079\n",
            "\t Val. Loss: 0.533 |  Val. PPL:   1.704\n",
            "Epoch: 101 | Time: 0m 4s\n",
            "\tTrain Loss: 0.075 | Train PPL:   1.078\n",
            "\t Val. Loss: 0.524 |  Val. PPL:   1.689\n",
            "Epoch: 102 | Time: 0m 4s\n",
            "\tTrain Loss: 0.073 | Train PPL:   1.075\n",
            "\t Val. Loss: 0.539 |  Val. PPL:   1.715\n",
            "Epoch: 103 | Time: 0m 4s\n",
            "\tTrain Loss: 0.073 | Train PPL:   1.075\n",
            "\t Val. Loss: 0.506 |  Val. PPL:   1.659\n",
            "Epoch: 104 | Time: 0m 4s\n",
            "\tTrain Loss: 0.072 | Train PPL:   1.074\n",
            "\t Val. Loss: 0.527 |  Val. PPL:   1.695\n",
            "Epoch: 105 | Time: 0m 4s\n",
            "\tTrain Loss: 0.073 | Train PPL:   1.075\n",
            "\t Val. Loss: 0.526 |  Val. PPL:   1.692\n",
            "Epoch: 106 | Time: 0m 4s\n",
            "\tTrain Loss: 0.069 | Train PPL:   1.072\n",
            "\t Val. Loss: 0.527 |  Val. PPL:   1.694\n",
            "Epoch: 107 | Time: 0m 4s\n",
            "\tTrain Loss: 0.070 | Train PPL:   1.072\n",
            "\t Val. Loss: 0.520 |  Val. PPL:   1.683\n",
            "Epoch: 108 | Time: 0m 4s\n",
            "\tTrain Loss: 0.068 | Train PPL:   1.071\n",
            "\t Val. Loss: 0.537 |  Val. PPL:   1.712\n",
            "Epoch: 109 | Time: 0m 4s\n",
            "\tTrain Loss: 0.070 | Train PPL:   1.072\n",
            "\t Val. Loss: 0.534 |  Val. PPL:   1.706\n",
            "Epoch: 110 | Time: 0m 4s\n",
            "\tTrain Loss: 0.066 | Train PPL:   1.068\n",
            "\t Val. Loss: 0.530 |  Val. PPL:   1.700\n",
            "Epoch: 111 | Time: 0m 4s\n",
            "\tTrain Loss: 0.065 | Train PPL:   1.067\n",
            "\t Val. Loss: 0.536 |  Val. PPL:   1.709\n",
            "Epoch: 112 | Time: 0m 4s\n",
            "\tTrain Loss: 0.065 | Train PPL:   1.067\n",
            "\t Val. Loss: 0.533 |  Val. PPL:   1.705\n",
            "Epoch: 113 | Time: 0m 4s\n",
            "\tTrain Loss: 0.065 | Train PPL:   1.068\n",
            "\t Val. Loss: 0.539 |  Val. PPL:   1.715\n",
            "Epoch: 114 | Time: 0m 4s\n",
            "\tTrain Loss: 0.064 | Train PPL:   1.066\n",
            "\t Val. Loss: 0.550 |  Val. PPL:   1.734\n",
            "Epoch: 115 | Time: 0m 4s\n",
            "\tTrain Loss: 0.062 | Train PPL:   1.064\n",
            "\t Val. Loss: 0.552 |  Val. PPL:   1.738\n",
            "Epoch: 116 | Time: 0m 4s\n",
            "\tTrain Loss: 0.062 | Train PPL:   1.064\n",
            "\t Val. Loss: 0.559 |  Val. PPL:   1.749\n",
            "Epoch: 117 | Time: 0m 4s\n",
            "\tTrain Loss: 0.058 | Train PPL:   1.060\n",
            "\t Val. Loss: 0.566 |  Val. PPL:   1.761\n",
            "Epoch: 118 | Time: 0m 4s\n",
            "\tTrain Loss: 0.060 | Train PPL:   1.061\n",
            "\t Val. Loss: 0.549 |  Val. PPL:   1.732\n",
            "Epoch: 119 | Time: 0m 4s\n",
            "\tTrain Loss: 0.059 | Train PPL:   1.061\n",
            "\t Val. Loss: 0.547 |  Val. PPL:   1.729\n",
            "Epoch: 120 | Time: 0m 4s\n",
            "\tTrain Loss: 0.058 | Train PPL:   1.060\n",
            "\t Val. Loss: 0.541 |  Val. PPL:   1.717\n",
            "Epoch: 121 | Time: 0m 4s\n",
            "\tTrain Loss: 0.058 | Train PPL:   1.059\n",
            "\t Val. Loss: 0.541 |  Val. PPL:   1.718\n",
            "Epoch: 122 | Time: 0m 4s\n",
            "\tTrain Loss: 0.059 | Train PPL:   1.061\n",
            "\t Val. Loss: 0.552 |  Val. PPL:   1.737\n",
            "Epoch: 123 | Time: 0m 4s\n",
            "\tTrain Loss: 0.055 | Train PPL:   1.057\n",
            "\t Val. Loss: 0.557 |  Val. PPL:   1.746\n",
            "Epoch: 124 | Time: 0m 4s\n",
            "\tTrain Loss: 0.054 | Train PPL:   1.056\n",
            "\t Val. Loss: 0.553 |  Val. PPL:   1.738\n",
            "Epoch: 125 | Time: 0m 4s\n",
            "\tTrain Loss: 0.055 | Train PPL:   1.056\n",
            "\t Val. Loss: 0.547 |  Val. PPL:   1.729\n",
            "Epoch: 126 | Time: 0m 4s\n",
            "\tTrain Loss: 0.054 | Train PPL:   1.056\n",
            "\t Val. Loss: 0.569 |  Val. PPL:   1.767\n",
            "Epoch: 127 | Time: 0m 4s\n",
            "\tTrain Loss: 0.054 | Train PPL:   1.055\n",
            "\t Val. Loss: 0.557 |  Val. PPL:   1.745\n",
            "Epoch: 128 | Time: 0m 4s\n",
            "\tTrain Loss: 0.054 | Train PPL:   1.055\n",
            "\t Val. Loss: 0.558 |  Val. PPL:   1.746\n",
            "Epoch: 129 | Time: 0m 4s\n",
            "\tTrain Loss: 0.053 | Train PPL:   1.054\n",
            "\t Val. Loss: 0.551 |  Val. PPL:   1.734\n",
            "Epoch: 130 | Time: 0m 4s\n",
            "\tTrain Loss: 0.052 | Train PPL:   1.053\n",
            "\t Val. Loss: 0.555 |  Val. PPL:   1.742\n",
            "Epoch: 131 | Time: 0m 4s\n",
            "\tTrain Loss: 0.053 | Train PPL:   1.054\n",
            "\t Val. Loss: 0.572 |  Val. PPL:   1.772\n",
            "Epoch: 132 | Time: 0m 4s\n",
            "\tTrain Loss: 0.050 | Train PPL:   1.051\n",
            "\t Val. Loss: 0.550 |  Val. PPL:   1.732\n",
            "Epoch: 133 | Time: 0m 4s\n",
            "\tTrain Loss: 0.051 | Train PPL:   1.052\n",
            "\t Val. Loss: 0.571 |  Val. PPL:   1.770\n",
            "Epoch: 134 | Time: 0m 4s\n",
            "\tTrain Loss: 0.050 | Train PPL:   1.052\n",
            "\t Val. Loss: 0.561 |  Val. PPL:   1.752\n",
            "Epoch: 135 | Time: 0m 4s\n",
            "\tTrain Loss: 0.050 | Train PPL:   1.051\n",
            "\t Val. Loss: 0.569 |  Val. PPL:   1.766\n",
            "Epoch: 136 | Time: 0m 4s\n",
            "\tTrain Loss: 0.048 | Train PPL:   1.049\n",
            "\t Val. Loss: 0.578 |  Val. PPL:   1.782\n",
            "Epoch: 137 | Time: 0m 4s\n",
            "\tTrain Loss: 0.047 | Train PPL:   1.048\n",
            "\t Val. Loss: 0.557 |  Val. PPL:   1.745\n",
            "Epoch: 138 | Time: 0m 4s\n",
            "\tTrain Loss: 0.049 | Train PPL:   1.050\n",
            "\t Val. Loss: 0.590 |  Val. PPL:   1.804\n",
            "Epoch: 139 | Time: 0m 4s\n",
            "\tTrain Loss: 0.048 | Train PPL:   1.050\n",
            "\t Val. Loss: 0.590 |  Val. PPL:   1.803\n",
            "Epoch: 140 | Time: 0m 4s\n",
            "\tTrain Loss: 0.047 | Train PPL:   1.048\n",
            "\t Val. Loss: 0.583 |  Val. PPL:   1.792\n",
            "Epoch: 141 | Time: 0m 4s\n",
            "\tTrain Loss: 0.046 | Train PPL:   1.047\n",
            "\t Val. Loss: 0.587 |  Val. PPL:   1.799\n",
            "Epoch: 142 | Time: 0m 4s\n",
            "\tTrain Loss: 0.046 | Train PPL:   1.047\n",
            "\t Val. Loss: 0.583 |  Val. PPL:   1.792\n",
            "Epoch: 143 | Time: 0m 4s\n",
            "\tTrain Loss: 0.045 | Train PPL:   1.046\n",
            "\t Val. Loss: 0.577 |  Val. PPL:   1.781\n",
            "Epoch: 144 | Time: 0m 4s\n",
            "\tTrain Loss: 0.046 | Train PPL:   1.047\n",
            "\t Val. Loss: 0.595 |  Val. PPL:   1.813\n",
            "Epoch: 145 | Time: 0m 4s\n",
            "\tTrain Loss: 0.046 | Train PPL:   1.047\n",
            "\t Val. Loss: 0.582 |  Val. PPL:   1.790\n",
            "Epoch: 146 | Time: 0m 4s\n",
            "\tTrain Loss: 0.045 | Train PPL:   1.046\n",
            "\t Val. Loss: 0.594 |  Val. PPL:   1.812\n",
            "Epoch: 147 | Time: 0m 4s\n",
            "\tTrain Loss: 0.046 | Train PPL:   1.047\n",
            "\t Val. Loss: 0.598 |  Val. PPL:   1.819\n",
            "Epoch: 148 | Time: 0m 4s\n",
            "\tTrain Loss: 0.044 | Train PPL:   1.045\n",
            "\t Val. Loss: 0.578 |  Val. PPL:   1.782\n",
            "Epoch: 149 | Time: 0m 4s\n",
            "\tTrain Loss: 0.043 | Train PPL:   1.044\n",
            "\t Val. Loss: 0.592 |  Val. PPL:   1.808\n",
            "Epoch: 150 | Time: 0m 4s\n",
            "\tTrain Loss: 0.043 | Train PPL:   1.044\n",
            "\t Val. Loss: 0.594 |  Val. PPL:   1.811\n",
            "Epoch: 151 | Time: 0m 4s\n",
            "\tTrain Loss: 0.042 | Train PPL:   1.043\n",
            "\t Val. Loss: 0.582 |  Val. PPL:   1.789\n",
            "Epoch: 152 | Time: 0m 4s\n",
            "\tTrain Loss: 0.042 | Train PPL:   1.043\n",
            "\t Val. Loss: 0.599 |  Val. PPL:   1.820\n",
            "Epoch: 153 | Time: 0m 4s\n",
            "\tTrain Loss: 0.042 | Train PPL:   1.043\n",
            "\t Val. Loss: 0.590 |  Val. PPL:   1.804\n",
            "Epoch: 154 | Time: 0m 4s\n",
            "\tTrain Loss: 0.042 | Train PPL:   1.043\n",
            "\t Val. Loss: 0.585 |  Val. PPL:   1.795\n",
            "Epoch: 155 | Time: 0m 4s\n",
            "\tTrain Loss: 0.042 | Train PPL:   1.043\n",
            "\t Val. Loss: 0.594 |  Val. PPL:   1.811\n",
            "Epoch: 156 | Time: 0m 4s\n",
            "\tTrain Loss: 0.042 | Train PPL:   1.043\n",
            "\t Val. Loss: 0.586 |  Val. PPL:   1.797\n",
            "Epoch: 157 | Time: 0m 4s\n",
            "\tTrain Loss: 0.041 | Train PPL:   1.041\n",
            "\t Val. Loss: 0.595 |  Val. PPL:   1.812\n",
            "Epoch: 158 | Time: 0m 4s\n",
            "\tTrain Loss: 0.041 | Train PPL:   1.042\n",
            "\t Val. Loss: 0.578 |  Val. PPL:   1.783\n",
            "Epoch: 159 | Time: 0m 4s\n",
            "\tTrain Loss: 0.040 | Train PPL:   1.041\n",
            "\t Val. Loss: 0.586 |  Val. PPL:   1.796\n",
            "Epoch: 160 | Time: 0m 4s\n",
            "\tTrain Loss: 0.039 | Train PPL:   1.039\n",
            "\t Val. Loss: 0.606 |  Val. PPL:   1.833\n",
            "Epoch: 161 | Time: 0m 4s\n",
            "\tTrain Loss: 0.038 | Train PPL:   1.039\n",
            "\t Val. Loss: 0.603 |  Val. PPL:   1.827\n",
            "Epoch: 162 | Time: 0m 4s\n",
            "\tTrain Loss: 0.040 | Train PPL:   1.041\n",
            "\t Val. Loss: 0.596 |  Val. PPL:   1.814\n",
            "Epoch: 163 | Time: 0m 4s\n",
            "\tTrain Loss: 0.037 | Train PPL:   1.038\n",
            "\t Val. Loss: 0.597 |  Val. PPL:   1.816\n",
            "Epoch: 164 | Time: 0m 4s\n",
            "\tTrain Loss: 0.038 | Train PPL:   1.039\n",
            "\t Val. Loss: 0.613 |  Val. PPL:   1.847\n",
            "Epoch: 165 | Time: 0m 4s\n",
            "\tTrain Loss: 0.037 | Train PPL:   1.038\n",
            "\t Val. Loss: 0.615 |  Val. PPL:   1.851\n",
            "Epoch: 166 | Time: 0m 4s\n",
            "\tTrain Loss: 0.037 | Train PPL:   1.038\n",
            "\t Val. Loss: 0.608 |  Val. PPL:   1.836\n",
            "Epoch: 167 | Time: 0m 4s\n",
            "\tTrain Loss: 0.036 | Train PPL:   1.037\n",
            "\t Val. Loss: 0.621 |  Val. PPL:   1.860\n",
            "Epoch: 168 | Time: 0m 4s\n",
            "\tTrain Loss: 0.037 | Train PPL:   1.038\n",
            "\t Val. Loss: 0.609 |  Val. PPL:   1.839\n",
            "Epoch: 169 | Time: 0m 4s\n",
            "\tTrain Loss: 0.036 | Train PPL:   1.037\n",
            "\t Val. Loss: 0.628 |  Val. PPL:   1.874\n",
            "Epoch: 170 | Time: 0m 4s\n",
            "\tTrain Loss: 0.037 | Train PPL:   1.037\n",
            "\t Val. Loss: 0.617 |  Val. PPL:   1.854\n",
            "Epoch: 171 | Time: 0m 4s\n",
            "\tTrain Loss: 0.036 | Train PPL:   1.036\n",
            "\t Val. Loss: 0.616 |  Val. PPL:   1.851\n",
            "Epoch: 172 | Time: 0m 4s\n",
            "\tTrain Loss: 0.036 | Train PPL:   1.036\n",
            "\t Val. Loss: 0.622 |  Val. PPL:   1.863\n",
            "Epoch: 173 | Time: 0m 4s\n",
            "\tTrain Loss: 0.035 | Train PPL:   1.035\n",
            "\t Val. Loss: 0.606 |  Val. PPL:   1.833\n",
            "Epoch: 174 | Time: 0m 4s\n",
            "\tTrain Loss: 0.035 | Train PPL:   1.036\n",
            "\t Val. Loss: 0.621 |  Val. PPL:   1.861\n",
            "Epoch: 175 | Time: 0m 4s\n",
            "\tTrain Loss: 0.035 | Train PPL:   1.036\n",
            "\t Val. Loss: 0.640 |  Val. PPL:   1.897\n",
            "Epoch: 176 | Time: 0m 4s\n",
            "\tTrain Loss: 0.034 | Train PPL:   1.034\n",
            "\t Val. Loss: 0.622 |  Val. PPL:   1.863\n",
            "Epoch: 177 | Time: 0m 4s\n",
            "\tTrain Loss: 0.033 | Train PPL:   1.034\n",
            "\t Val. Loss: 0.619 |  Val. PPL:   1.857\n",
            "Epoch: 178 | Time: 0m 4s\n",
            "\tTrain Loss: 0.034 | Train PPL:   1.035\n",
            "\t Val. Loss: 0.626 |  Val. PPL:   1.871\n",
            "Epoch: 179 | Time: 0m 4s\n",
            "\tTrain Loss: 0.034 | Train PPL:   1.035\n",
            "\t Val. Loss: 0.618 |  Val. PPL:   1.856\n",
            "Epoch: 180 | Time: 0m 4s\n",
            "\tTrain Loss: 0.034 | Train PPL:   1.034\n",
            "\t Val. Loss: 0.616 |  Val. PPL:   1.851\n",
            "Epoch: 181 | Time: 0m 4s\n",
            "\tTrain Loss: 0.033 | Train PPL:   1.034\n",
            "\t Val. Loss: 0.621 |  Val. PPL:   1.861\n",
            "Epoch: 182 | Time: 0m 4s\n",
            "\tTrain Loss: 0.032 | Train PPL:   1.033\n",
            "\t Val. Loss: 0.628 |  Val. PPL:   1.874\n",
            "Epoch: 183 | Time: 0m 4s\n",
            "\tTrain Loss: 0.032 | Train PPL:   1.032\n",
            "\t Val. Loss: 0.625 |  Val. PPL:   1.868\n",
            "Epoch: 184 | Time: 0m 4s\n",
            "\tTrain Loss: 0.032 | Train PPL:   1.032\n",
            "\t Val. Loss: 0.643 |  Val. PPL:   1.903\n",
            "Epoch: 185 | Time: 0m 4s\n",
            "\tTrain Loss: 0.033 | Train PPL:   1.034\n",
            "\t Val. Loss: 0.642 |  Val. PPL:   1.900\n",
            "Epoch: 186 | Time: 0m 4s\n",
            "\tTrain Loss: 0.032 | Train PPL:   1.032\n",
            "\t Val. Loss: 0.645 |  Val. PPL:   1.906\n",
            "Epoch: 187 | Time: 0m 4s\n",
            "\tTrain Loss: 0.033 | Train PPL:   1.033\n",
            "\t Val. Loss: 0.640 |  Val. PPL:   1.897\n",
            "Epoch: 188 | Time: 0m 4s\n",
            "\tTrain Loss: 0.032 | Train PPL:   1.033\n",
            "\t Val. Loss: 0.640 |  Val. PPL:   1.896\n",
            "Epoch: 189 | Time: 0m 4s\n",
            "\tTrain Loss: 0.031 | Train PPL:   1.032\n",
            "\t Val. Loss: 0.647 |  Val. PPL:   1.909\n",
            "Epoch: 190 | Time: 0m 4s\n",
            "\tTrain Loss: 0.031 | Train PPL:   1.032\n",
            "\t Val. Loss: 0.636 |  Val. PPL:   1.889\n",
            "Epoch: 191 | Time: 0m 4s\n",
            "\tTrain Loss: 0.031 | Train PPL:   1.031\n",
            "\t Val. Loss: 0.650 |  Val. PPL:   1.915\n",
            "Epoch: 192 | Time: 0m 4s\n",
            "\tTrain Loss: 0.030 | Train PPL:   1.030\n",
            "\t Val. Loss: 0.653 |  Val. PPL:   1.921\n",
            "Epoch: 193 | Time: 0m 4s\n",
            "\tTrain Loss: 0.031 | Train PPL:   1.031\n",
            "\t Val. Loss: 0.640 |  Val. PPL:   1.897\n",
            "Epoch: 194 | Time: 0m 4s\n",
            "\tTrain Loss: 0.031 | Train PPL:   1.031\n",
            "\t Val. Loss: 0.649 |  Val. PPL:   1.914\n",
            "Epoch: 195 | Time: 0m 4s\n",
            "\tTrain Loss: 0.032 | Train PPL:   1.033\n",
            "\t Val. Loss: 0.642 |  Val. PPL:   1.900\n",
            "Epoch: 196 | Time: 0m 4s\n",
            "\tTrain Loss: 0.030 | Train PPL:   1.031\n",
            "\t Val. Loss: 0.652 |  Val. PPL:   1.920\n",
            "Epoch: 197 | Time: 0m 4s\n",
            "\tTrain Loss: 0.029 | Train PPL:   1.030\n",
            "\t Val. Loss: 0.657 |  Val. PPL:   1.928\n",
            "Epoch: 198 | Time: 0m 4s\n",
            "\tTrain Loss: 0.029 | Train PPL:   1.030\n",
            "\t Val. Loss: 0.648 |  Val. PPL:   1.911\n",
            "Epoch: 199 | Time: 0m 4s\n",
            "\tTrain Loss: 0.031 | Train PPL:   1.031\n",
            "\t Val. Loss: 0.653 |  Val. PPL:   1.922\n",
            "Epoch: 200 | Time: 0m 4s\n",
            "\tTrain Loss: 0.029 | Train PPL:   1.029\n",
            "\t Val. Loss: 0.645 |  Val. PPL:   1.906\n",
            "Epoch: 201 | Time: 0m 4s\n",
            "\tTrain Loss: 0.029 | Train PPL:   1.030\n",
            "\t Val. Loss: 0.659 |  Val. PPL:   1.933\n",
            "Epoch: 202 | Time: 0m 4s\n",
            "\tTrain Loss: 0.030 | Train PPL:   1.031\n",
            "\t Val. Loss: 0.659 |  Val. PPL:   1.933\n",
            "Epoch: 203 | Time: 0m 4s\n",
            "\tTrain Loss: 0.027 | Train PPL:   1.028\n",
            "\t Val. Loss: 0.642 |  Val. PPL:   1.901\n",
            "Epoch: 204 | Time: 0m 4s\n",
            "\tTrain Loss: 0.028 | Train PPL:   1.029\n",
            "\t Val. Loss: 0.651 |  Val. PPL:   1.917\n",
            "Epoch: 205 | Time: 0m 4s\n",
            "\tTrain Loss: 0.028 | Train PPL:   1.028\n",
            "\t Val. Loss: 0.656 |  Val. PPL:   1.926\n",
            "Epoch: 206 | Time: 0m 4s\n",
            "\tTrain Loss: 0.029 | Train PPL:   1.029\n",
            "\t Val. Loss: 0.637 |  Val. PPL:   1.890\n",
            "Epoch: 207 | Time: 0m 4s\n",
            "\tTrain Loss: 0.029 | Train PPL:   1.029\n",
            "\t Val. Loss: 0.654 |  Val. PPL:   1.923\n",
            "Epoch: 208 | Time: 0m 4s\n",
            "\tTrain Loss: 0.028 | Train PPL:   1.028\n",
            "\t Val. Loss: 0.649 |  Val. PPL:   1.913\n",
            "Epoch: 209 | Time: 0m 4s\n",
            "\tTrain Loss: 0.029 | Train PPL:   1.030\n",
            "\t Val. Loss: 0.666 |  Val. PPL:   1.946\n",
            "Epoch: 210 | Time: 0m 4s\n",
            "\tTrain Loss: 0.028 | Train PPL:   1.028\n",
            "\t Val. Loss: 0.661 |  Val. PPL:   1.937\n",
            "Epoch: 211 | Time: 0m 4s\n",
            "\tTrain Loss: 0.028 | Train PPL:   1.029\n",
            "\t Val. Loss: 0.665 |  Val. PPL:   1.944\n",
            "Epoch: 212 | Time: 0m 4s\n",
            "\tTrain Loss: 0.028 | Train PPL:   1.028\n",
            "\t Val. Loss: 0.680 |  Val. PPL:   1.974\n",
            "Epoch: 213 | Time: 0m 4s\n",
            "\tTrain Loss: 0.028 | Train PPL:   1.029\n",
            "\t Val. Loss: 0.668 |  Val. PPL:   1.950\n",
            "Epoch: 214 | Time: 0m 4s\n",
            "\tTrain Loss: 0.027 | Train PPL:   1.027\n",
            "\t Val. Loss: 0.680 |  Val. PPL:   1.973\n",
            "Epoch: 215 | Time: 0m 4s\n",
            "\tTrain Loss: 0.029 | Train PPL:   1.029\n",
            "\t Val. Loss: 0.678 |  Val. PPL:   1.970\n",
            "Epoch: 216 | Time: 0m 4s\n",
            "\tTrain Loss: 0.028 | Train PPL:   1.028\n",
            "\t Val. Loss: 0.664 |  Val. PPL:   1.942\n",
            "Epoch: 217 | Time: 0m 4s\n",
            "\tTrain Loss: 0.027 | Train PPL:   1.027\n",
            "\t Val. Loss: 0.650 |  Val. PPL:   1.916\n",
            "Epoch: 218 | Time: 0m 4s\n",
            "\tTrain Loss: 0.028 | Train PPL:   1.028\n",
            "\t Val. Loss: 0.675 |  Val. PPL:   1.963\n",
            "Epoch: 219 | Time: 0m 4s\n",
            "\tTrain Loss: 0.027 | Train PPL:   1.027\n",
            "\t Val. Loss: 0.683 |  Val. PPL:   1.980\n",
            "Epoch: 220 | Time: 0m 4s\n",
            "\tTrain Loss: 0.027 | Train PPL:   1.028\n",
            "\t Val. Loss: 0.668 |  Val. PPL:   1.950\n",
            "Epoch: 221 | Time: 0m 4s\n",
            "\tTrain Loss: 0.027 | Train PPL:   1.027\n",
            "\t Val. Loss: 0.681 |  Val. PPL:   1.976\n",
            "Epoch: 222 | Time: 0m 4s\n",
            "\tTrain Loss: 0.027 | Train PPL:   1.027\n",
            "\t Val. Loss: 0.660 |  Val. PPL:   1.935\n",
            "Epoch: 223 | Time: 0m 4s\n",
            "\tTrain Loss: 0.026 | Train PPL:   1.027\n",
            "\t Val. Loss: 0.666 |  Val. PPL:   1.946\n",
            "Epoch: 224 | Time: 0m 4s\n",
            "\tTrain Loss: 0.027 | Train PPL:   1.027\n",
            "\t Val. Loss: 0.674 |  Val. PPL:   1.962\n",
            "Epoch: 225 | Time: 0m 4s\n",
            "\tTrain Loss: 0.027 | Train PPL:   1.027\n",
            "\t Val. Loss: 0.682 |  Val. PPL:   1.978\n",
            "Epoch: 226 | Time: 0m 4s\n",
            "\tTrain Loss: 0.026 | Train PPL:   1.026\n",
            "\t Val. Loss: 0.681 |  Val. PPL:   1.976\n",
            "Epoch: 227 | Time: 0m 4s\n",
            "\tTrain Loss: 0.027 | Train PPL:   1.027\n",
            "\t Val. Loss: 0.682 |  Val. PPL:   1.978\n",
            "Epoch: 228 | Time: 0m 4s\n",
            "\tTrain Loss: 0.027 | Train PPL:   1.027\n",
            "\t Val. Loss: 0.666 |  Val. PPL:   1.946\n",
            "Epoch: 229 | Time: 0m 4s\n",
            "\tTrain Loss: 0.025 | Train PPL:   1.025\n",
            "\t Val. Loss: 0.673 |  Val. PPL:   1.961\n",
            "Epoch: 230 | Time: 0m 4s\n",
            "\tTrain Loss: 0.026 | Train PPL:   1.027\n",
            "\t Val. Loss: 0.695 |  Val. PPL:   2.003\n",
            "Epoch: 231 | Time: 0m 4s\n",
            "\tTrain Loss: 0.026 | Train PPL:   1.026\n",
            "\t Val. Loss: 0.690 |  Val. PPL:   1.993\n",
            "Epoch: 232 | Time: 0m 4s\n",
            "\tTrain Loss: 0.026 | Train PPL:   1.026\n",
            "\t Val. Loss: 0.685 |  Val. PPL:   1.983\n",
            "Epoch: 233 | Time: 0m 4s\n",
            "\tTrain Loss: 0.026 | Train PPL:   1.026\n",
            "\t Val. Loss: 0.680 |  Val. PPL:   1.973\n",
            "Epoch: 234 | Time: 0m 4s\n",
            "\tTrain Loss: 0.027 | Train PPL:   1.028\n",
            "\t Val. Loss: 0.680 |  Val. PPL:   1.975\n",
            "Epoch: 235 | Time: 0m 4s\n",
            "\tTrain Loss: 0.025 | Train PPL:   1.025\n",
            "\t Val. Loss: 0.682 |  Val. PPL:   1.977\n",
            "Epoch: 236 | Time: 0m 4s\n",
            "\tTrain Loss: 0.025 | Train PPL:   1.025\n",
            "\t Val. Loss: 0.674 |  Val. PPL:   1.963\n",
            "Epoch: 237 | Time: 0m 4s\n",
            "\tTrain Loss: 0.023 | Train PPL:   1.024\n",
            "\t Val. Loss: 0.688 |  Val. PPL:   1.989\n",
            "Epoch: 238 | Time: 0m 4s\n",
            "\tTrain Loss: 0.025 | Train PPL:   1.025\n",
            "\t Val. Loss: 0.701 |  Val. PPL:   2.015\n",
            "Epoch: 239 | Time: 0m 4s\n",
            "\tTrain Loss: 0.025 | Train PPL:   1.025\n",
            "\t Val. Loss: 0.690 |  Val. PPL:   1.993\n",
            "Epoch: 240 | Time: 0m 4s\n",
            "\tTrain Loss: 0.024 | Train PPL:   1.025\n",
            "\t Val. Loss: 0.678 |  Val. PPL:   1.969\n",
            "Epoch: 241 | Time: 0m 4s\n",
            "\tTrain Loss: 0.025 | Train PPL:   1.025\n",
            "\t Val. Loss: 0.688 |  Val. PPL:   1.990\n",
            "Epoch: 242 | Time: 0m 4s\n",
            "\tTrain Loss: 0.026 | Train PPL:   1.026\n",
            "\t Val. Loss: 0.691 |  Val. PPL:   1.995\n",
            "Epoch: 243 | Time: 0m 4s\n",
            "\tTrain Loss: 0.025 | Train PPL:   1.026\n",
            "\t Val. Loss: 0.689 |  Val. PPL:   1.992\n",
            "Epoch: 244 | Time: 0m 4s\n",
            "\tTrain Loss: 0.024 | Train PPL:   1.025\n",
            "\t Val. Loss: 0.710 |  Val. PPL:   2.033\n",
            "Epoch: 245 | Time: 0m 4s\n",
            "\tTrain Loss: 0.023 | Train PPL:   1.023\n",
            "\t Val. Loss: 0.684 |  Val. PPL:   1.982\n",
            "Epoch: 246 | Time: 0m 4s\n",
            "\tTrain Loss: 0.024 | Train PPL:   1.024\n",
            "\t Val. Loss: 0.670 |  Val. PPL:   1.954\n",
            "Epoch: 247 | Time: 0m 4s\n",
            "\tTrain Loss: 0.024 | Train PPL:   1.025\n",
            "\t Val. Loss: 0.699 |  Val. PPL:   2.012\n",
            "Epoch: 248 | Time: 0m 4s\n",
            "\tTrain Loss: 0.024 | Train PPL:   1.024\n",
            "\t Val. Loss: 0.681 |  Val. PPL:   1.976\n",
            "Epoch: 249 | Time: 0m 4s\n",
            "\tTrain Loss: 0.024 | Train PPL:   1.024\n",
            "\t Val. Loss: 0.690 |  Val. PPL:   1.993\n",
            "Epoch: 250 | Time: 0m 4s\n",
            "\tTrain Loss: 0.024 | Train PPL:   1.024\n",
            "\t Val. Loss: 0.674 |  Val. PPL:   1.961\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kbCGyO4ZEzFm"
      },
      "source": [
        "Then, we train our model, saving the parameters that give us the best validation loss."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ztR5mNm8EzFn"
      },
      "source": [
        "Finally, we test the model on the test set using these \"best\" parameters."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b5F5wPqT60FC",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "72c530dc-d744-441e-81da-f98e36f20229"
      },
      "source": [
        "model.load_state_dict(torch.load('english_python.pt'))\n",
        "\n",
        "valid_loss = evaluate(model, valid_iterator, criterion)\n",
        "\n",
        "print(f'| Validation Loss: {valid_loss:.3f} ')"
      ],
      "execution_count": 79,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "| Validation Loss: 0.485 \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q5TItCLNV57f"
      },
      "source": [
        "def generate_program(sentence, src_field, trg_field, model, device, max_len = 50):\n",
        "    \n",
        "    model.eval()\n",
        "        \n",
        "    if isinstance(sentence, str):\n",
        "        nlp = spacy.load('en')\n",
        "        tokens = [token.text.lower() for token in nlp(sentence)]\n",
        "    else:\n",
        "        tokens = [token.lower() for token in sentence]\n",
        "\n",
        "    tokens = [src_field.init_token] + tokens + [src_field.eos_token]\n",
        "        \n",
        "    src_indexes = [src_field.vocab.stoi[token] for token in tokens]\n",
        "\n",
        "    src_tensor = torch.LongTensor(src_indexes).unsqueeze(0).to(device)\n",
        "    \n",
        "    src_mask = model.make_src_mask(src_tensor)\n",
        "    \n",
        "    with torch.no_grad():\n",
        "        enc_src = model.encoder(src_tensor, src_mask)\n",
        "\n",
        "    trg_indexes = [trg_field.vocab.stoi[trg_field.init_token]]\n",
        "\n",
        "    for i in range(max_len):\n",
        "\n",
        "        trg_tensor = torch.LongTensor(trg_indexes).unsqueeze(0).to(device)\n",
        "\n",
        "        trg_mask = model.make_trg_mask(trg_tensor)\n",
        "        \n",
        "        with torch.no_grad():\n",
        "            output, attention = model.decoder(trg_tensor, enc_src, trg_mask, src_mask)\n",
        "        \n",
        "        pred_token = output.argmax(2)[:,-1].item()\n",
        "        \n",
        "        trg_indexes.append(pred_token)\n",
        "\n",
        "        if pred_token == trg_field.vocab.stoi[trg_field.eos_token]:\n",
        "            break\n",
        "    \n",
        "    trg_tokens = [trg_field.vocab.itos[i] for i in trg_indexes]\n",
        "    \n",
        "    return trg_tokens[1:], attention"
      ],
      "execution_count": 80,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "otFG6WuZWBTN"
      },
      "source": [
        "def display_attention(sentence, translation, attention, n_heads = 8, n_rows = 4, n_cols = 2):\n",
        "    \n",
        "    assert n_rows * n_cols == n_heads\n",
        "    \n",
        "    fig = plt.figure(figsize=(15,25))\n",
        "    \n",
        "    for i in range(n_heads):\n",
        "        \n",
        "        ax = fig.add_subplot(n_rows, n_cols, i+1)\n",
        "        \n",
        "        _attention = attention.squeeze(0)[i].cpu().detach().numpy()\n",
        "\n",
        "        cax = ax.matshow(_attention, cmap='bone')\n",
        "\n",
        "        ax.tick_params(labelsize=12)\n",
        "        ax.set_xticklabels(['']+['<sos>']+[t.lower() for t in sentence]+['<eos>'], \n",
        "                           rotation=45)\n",
        "        ax.set_yticklabels(['']+translation)\n",
        "\n",
        "        ax.xaxis.set_major_locator(ticker.MultipleLocator(1))\n",
        "        ax.yaxis.set_major_locator(ticker.MultipleLocator(1))\n",
        "\n",
        "    plt.show()"
      ],
      "execution_count": 81,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t9-B6GtsxwoR"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SGkRVPXrWVRv",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d0da1f5a-7781-4d2d-c8b0-208706452a18"
      },
      "source": [
        "example_idx = 5\n",
        "\n",
        "src = vars(valid_dataset.examples[example_idx])['English']\n",
        "trg = vars(valid_dataset.examples[example_idx])['Python']\n",
        "src_final = \" \".join(src)\n",
        "trg_final = \" \".join(trg)\n",
        "\n",
        "\n",
        "\n",
        "trg_final = trg_final.replace('NEWLINE','\\n')\n",
        "trg_final = trg_final.replace('INDENT','\\t')\n",
        "trg_final = trg_final.replace('DEDENT',' ')\n",
        "print(src_final)\n",
        "print(trg_final)\n"
      ],
      "execution_count": 82,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "write a python function to append all elements of one list to another\n",
            "def extend_list ( list1 , list2 ) : \n",
            " \t list1 = [ 1 , 2 ] \n",
            " list2 = [ 3 , 4 ] \n",
            " return list1 . extend ( list2 ) \n",
            "  \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lkMKpWU3W7CQ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ef55f6b6-f4e0-4d03-e09b-8c10e6672060"
      },
      "source": [
        "example_idx = 2\n",
        "src = vars(valid_dataset.examples[example_idx])['English']\n",
        "#src =  src.split()\n",
        "src_final = \" \".join(src)\n",
        "print(src_final)\n",
        "\n",
        "trg, attention = generate_program(src_final, SRC, TRG, model, device)\n",
        "trg_final = \" \".join(trg)\n",
        "trg_final = trg_final.replace('NEWLINE','\\n')\n",
        "trg_final = trg_final.replace('INDENT','\\t')\n",
        "\n",
        "print(trg_final)"
      ],
      "execution_count": 83,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "write a python program to print a list after removing elements from index 1 to 4\n",
            "list1 = [ 11 , 5 , 17 , 18 , 23 , 50 ] \n",
            " del list1 [ 1 : 5 ] \n",
            " print ( * list1 ) \n",
            " <eos>\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FeYMv9PFrq9K",
        "outputId": "c85e2772-8a18-46e7-debd-daa86b400cfb"
      },
      "source": [
        "example_idx = 1\n",
        "src = vars(valid_dataset.examples[example_idx])['English']\n",
        "#src =  src.split()\n",
        "src_final = \" \".join(src)\n",
        "print(src_final)\n",
        "\n",
        "trg, attention = generate_program(src_final, SRC, TRG, model, device)\n",
        "trg_final = \" \".join(trg)\n",
        "trg_final = trg_final.replace('NEWLINE','\\n')\n",
        "trg_final = trg_final.replace('INDENT','\\t')\n",
        "\n",
        "print(trg_final)"
      ],
      "execution_count": 84,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "write a python function to print pyramid pattern\n",
            "def pyramid_pattern ( symbol = '*' , count = 4 ) : \n",
            " \t for i in range ( 1 , count + 1 ) : \n",
            " \t print ( ' ' * ( count - i ) + symbol * i , end = '' ) \n",
            " print ( symbol\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zwqcuHl1ru6r",
        "outputId": "a63c6f1a-2808-4e8f-a3d6-01b42edd8b33"
      },
      "source": [
        "example_idx = 3\n",
        "src = vars(valid_dataset.examples[example_idx])['English']\n",
        "#src =  src.split()\n",
        "src_final = \" \".join(src)\n",
        "print(src_final)\n",
        "\n",
        "trg, attention = generate_program(src_final, SRC, TRG, model, device)\n",
        "trg_final = \" \".join(trg)\n",
        "trg_final = trg_final.replace('NEWLINE','\\n')\n",
        "trg_final = trg_final.replace('INDENT','\\t')\n",
        "\n",
        "print(trg_final)"
      ],
      "execution_count": 85,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "97 write a python function that accepts a number and returns the nearest square number\n",
            "import math \n",
            " def nearest_square ( n ) : \n",
            " \t upp = math . floor ( math . sqrt ( n ) ) \n",
            " low = math . floor ( math . sqrt ( n ) ) \n",
            " upp_diff = upp ** 2 - low ** 2 \n",
            " low_diff\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q0i_0KwbXWCz",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a9c7641e-87fe-4f38-98f5-c62c75a1a634"
      },
      "source": [
        "example_idx = 4\n",
        "src = vars(valid_dataset.examples[example_idx])['English']\n",
        "#src =  src.split()\n",
        "src_final = \" \".join(src)\n",
        "print(src_final)\n",
        "\n",
        "trg, attention = generate_program(src_final, SRC, TRG, model, device)\n",
        "trg_final = \" \".join(trg)\n",
        "trg_final = trg_final.replace('NEWLINE','\\n')\n",
        "trg_final = trg_final.replace('INDENT','\\t')\n",
        "\n",
        "print(trg_final)"
      ],
      "execution_count": 86,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "write a python program to make use of enumerate method\n",
            "q = [ 1 , 2 , 3 , 4 ] \n",
            " q . insert ( 0 , 5 ) \n",
            " print ( f\"Revised List:{q}\" ) \n",
            " <eos>\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qqlNsBZ5XZv8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "98c37ce9-630a-4fe9-dd62-1a0efd85d085"
      },
      "source": [
        "example_idx = 5\n",
        "src = vars(valid_dataset.examples[example_idx])['English']\n",
        "#src =  src.split()\n",
        "src_final = \" \".join(src)\n",
        "print(src_final)\n",
        "\n",
        "trg, attention = generate_program(src_final, SRC, TRG, model, device)\n",
        "trg_final = \" \".join(trg)\n",
        "trg_final = trg_final.replace('NEWLINE','\\n')\n",
        "trg_final = trg_final.replace('INDENT','\\t')\n",
        "\n",
        "print(trg_final)"
      ],
      "execution_count": 87,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "write a python function to append all elements of one list to another\n",
            "def extend_list ( list1 , list2 ) : \n",
            " \t list1 = [ 1 , 2 ] \n",
            " list2 = [ 3 , 4 ] \n",
            " return list1 . extend ( list2 ) \n",
            " DEDENT <eos>\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NCHOH1ZDbxmy",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1e11200d-77ab-454f-d352-674b8b2877f0"
      },
      "source": [
        "example_idx = 7\n",
        "src = vars(valid_dataset.examples[example_idx])['English']\n",
        "#src =  src.split()\n",
        "src_final = \" \".join(src)\n",
        "print(src_final)\n",
        "\n",
        "trg, attention = generate_program(src_final, SRC, TRG, model, device)\n",
        "trg_final = \" \".join(trg)\n",
        "trg_final = trg_final.replace('NEWLINE','\\n')\n",
        "trg_final = trg_final.replace('INDENT','\\t')\n",
        "\n",
        "print(trg_final)"
      ],
      "execution_count": 88,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "write a function that given a number find the most significant bit number which is set bit and which is in power of two\n",
            "from math import log \n",
            " def near_thousand ( n ) : \n",
            " \t return ( abs ( 1000 - x1 ) <= 100 ) \n",
            " DEDENT print ( near_thousand ( 'c' ) ) \n",
            " print ( near_thousand ( 900 ) ) \n",
            " print ( near_thousand ( 900 ) ) \n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "StHqf59oscWK",
        "outputId": "96fa4de0-d7e1-442f-dc61-d951df143dee"
      },
      "source": [
        "for example_idx in range(7,25):\n",
        "  src = vars(valid_dataset.examples[example_idx])['English']\n",
        "  #src =  src.split()\n",
        "  src_final = \" \".join(src)\n",
        "  print(src_final)\n",
        "\n",
        "  trg, attention = generate_program(src_final, SRC, TRG, model, device)\n",
        "  trg_final = \" \".join(trg)\n",
        "  trg_final = trg_final.replace('NEWLINE','\\n')\n",
        "  trg_final = trg_final.replace('INDENT','\\t')\n",
        "  trg_final = trg_final.replace('DEDENT ',' ')\n",
        "\n",
        "  print(trg_final)\n",
        "  print(\"\\n\\n\")"
      ],
      "execution_count": 92,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "write a function that given a number find the most significant bit number which is set bit and which is in power of two\n",
            "from math import log \n",
            " def near_thousand ( n ) : \n",
            " \t return ( abs ( 1000 - x1 ) <= 100 ) \n",
            "  print ( near_thousand ( 'c' ) ) \n",
            " print ( near_thousand ( 900 ) ) \n",
            " print ( near_thousand ( 900 ) ) \n",
            "\n",
            "\n",
            "\n",
            "\n",
            "write a function to rotate string left by a given length\n",
            "def rotate_left ( input , d ) : \n",
            " \t Lfirst = input [ 0 : d ] \n",
            " Lsecond = input [ d : ] \n",
            " return ( Lsecond + Lfirst ) \n",
            "  <eos>\n",
            "\n",
            "\n",
            "\n",
            "41 write a python program to check if one tuple is subset of other and print it\n",
            "def filter_with_key_value ( list_of_dicts , key , value ) : \n",
            " \t return list ( filter ( lambda x : x . get ( key ) == value , list_of_dicts ) ) \n",
            "  <eos>\n",
            "\n",
            "\n",
            "\n",
            "write a python program to get numbers divisible by fifteen from a list\n",
            "num_list = [ 45 , 55 , 60 , 37 , 100 , 105 , 220 ] \n",
            " result = list ( filter ( lambda x : ( x % 15 == 0 ) , num_list ) ) \n",
            " print ( f\"Numbers divisible by 15 are {result}\" ) \n",
            " <eos>\n",
            "\n",
            "\n",
            "\n",
            "calculate the sum of every pair of numbers from two lists\n",
            "list1 = [ 1 , 2 , 3 ] \n",
            " list2 = [ 5 , 6 , 7 ] \n",
            " final = [ a + b for a in list1 for b in list2 ] \n",
            " print ( f\"sum of every pair of numbers from two lists:{final}\" ) \n",
            " <eos>\n",
            "\n",
            "\n",
            "\n",
            "write python3 code to demonstrate conversion of list of tuple to list of list using map join\n",
            "test_list = [ ( 'G' , 'E' , 'E' , 'K' , 'S' ) , ( 'F' , 'O' , 'R' ) , \n",
            " ( 'G' , 'E' , 'E' , 'K' , 'S' ) ] \n",
            " print ( \"The original list is : \" + str ( test_list ) ) \n",
            " res = list\n",
            "\n",
            "\n",
            "\n",
            "write a program that iterates over a dictionary and prints bingo if length of value is greater than the length of key otherwise print no bingo\n",
            "key_val_map = { \"key1\" : \"length1\" , \"key2\" : \"len2\" , \"Hello\" : \"hi\" , \"bingo\" : \"print bingo\" } \n",
            " for key , val in key_val_map . items ( ) : \n",
            " \t if len ( val ) > len ( key ) : \n",
            " \t print ( \"Bingo!\" )\n",
            "\n",
            "\n",
            "\n",
            "write a python program to swap tuple elements in list of tuples print the output\n",
            "test_list = [ ( 3 , 4 ) , ( 6 , 5 ) , ( 7 , 8 ) , ( 2 , 4 , 5 ) ] \n",
            " K = 1 \n",
            " res = [ ele for ele in test_list if ele != K ] \n",
            " print (\n",
            "\n",
            "\n",
            "\n",
            "write a python function that takes in a list and returns a list containing the squares of the elements of the input list\n",
            "def square_list_elements ( list_to_be_squared ) : \n",
            " \t return list ( map ( lambda x : x ** 2 , list_to_be_squared ) ) \n",
            "  <eos>\n",
            "\n",
            "\n",
            "\n",
            "write a python function that accepts a dictionary that has unique values and returns its inversion\n",
            "def invert_dict ( input_dict ) : \n",
            " \t my_inverted_dict = { value : key for key , value in input_dict . items ( ) } \n",
            " return my_inverted_dict \n",
            "  <eos>\n",
            "\n",
            "\n",
            "\n",
            "shuffle a list randomly\n",
            "import random \n",
            " list = [ 2 , 5 , 8 , 9 , 12 ] \n",
            " random . shuffle ( list ) \n",
            " print ( \"Printing shuffled list \" , list ) \n",
            " <eos>\n",
            "\n",
            "\n",
            "\n",
            "write a function to compute 50 and use tryexcept to catch the exceptions\n",
            "def throws ( ) : \n",
            " \t return 5 / 0 \n",
            "  try : \n",
            " \t throws ( ) \n",
            "  except ZeroDivisionError : \n",
            " \t print ( \"division by zero!\" ) \n",
            "  except Exception : \n",
            " \t print ( 'Caught an exception' ) \n",
            "  finally : \n",
            " \t print (\n",
            "\n",
            "\n",
            "\n",
            "write a function to print a string by repeating it n times\n",
            "def print_repeatnstring ( text : str , n : int ) -> str : \n",
            " \t return text * n \n",
            "  <eos>\n",
            "\n",
            "\n",
            "\n",
            "join two sets\n",
            "set1 = { \"a\" , \"b\" , \"c\" } \n",
            " set2 = { 1 , 2 , 3 } \n",
            " set3 = set2 . union ( set1 ) \n",
            " print ( f\"Joined Set:{set3}\" ) \n",
            " <eos>\n",
            "\n",
            "\n",
            "\n",
            "write a python function to remove falsy values from a list\n",
            "def ascending_sort ( l : list ) : \n",
            " \t sorted ( l , reverse = False ) \n",
            "  <eos>\n",
            "\n",
            "\n",
            "\n",
            "write a function to return the total surface area of a cuboid of length l bredth b and height h\n",
            "def cal_surface_area_cuboid ( l , b , h ) : \n",
            " \t return 2 * ( l * b + b * h + h * l ) \n",
            "  <eos>\n",
            "\n",
            "\n",
            "\n",
            "write a program to print inverted star pattern for the given number\n",
            "n = int ( input ( \"Enter number of rows: \" ) ) \n",
            " for i in range ( n , 0 , - 1 ) : \n",
            " \t print ( ( n - i ) * ' ' + i * '*' ) \n",
            "  <eos>\n",
            "\n",
            "\n",
            "\n",
            "write a python program to read and print the contents of a file\n",
            "a = str ( input ( \"Enter file name .txt extension:\" ) ) \n",
            " file2 = open ( a , 'r' ) \n",
            " line = file2 . readline ( ) \n",
            " while ( line != \"\" ) : \n",
            " \t print ( line ) \n",
            " line = file2 . readline ( ) \n",
            " DEDENT\n",
            "\n",
            "\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H-rU99IysPR1"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}